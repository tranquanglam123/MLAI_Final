2023/05/23 07:07:20 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0]
    CUDA available: True
    numpy_random_seed: 48
    GPU 0: Tesla T4
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 11.8, V11.8.89
    GCC: x86_64-linux-gnu-gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
    PyTorch: 2.0.1+cu118
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.7
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

    TorchVision: 0.15.2+cu118
    OpenCV: 4.7.0
    MMEngine: 0.7.3

Runtime environment:
    cudnn_benchmark: True
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 48
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

2023/05/23 07:07:21 - mmengine - INFO - Config:
default_scope = 'mmseg'
env_cfg = dict(
    cudnn_benchmark=True,
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
    dist_cfg=dict(backend='nccl'))
vis_backends = [dict(type='LocalVisBackend')]
visualizer = dict(
    type='SegLocalVisualizer',
    vis_backends=[dict(type='LocalVisBackend')],
    name='visualizer')
log_processor = dict(by_epoch=True)
log_level = 'INFO'
load_from = 'https://download.openmmlab.com/mmsegmentation/v0.5/mask2former/mask2former_r50_8xb2-160k_ade20k-512x512/mask2former_r50_8xb2-160k_ade20k-512x512_20221204_000055-2d1f55f1.pth'
resume = False
tta_model = dict(type='SegTTAModel')
dataset_type = 'ISICTask1Dataset'
data_root = '/content/dataset'
crop_size = (512, 512)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PackSegInputs')
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='PackSegInputs')
]
img_ratios = [0.5, 0.75, 1.0, 1.25, 1.5, 1.75]
tta_pipeline = [
    dict(type='LoadImageFromFile', backend_args=None),
    dict(
        type='TestTimeAug',
        transforms=[[{
            'type': 'Resize',
            'scale_factor': 0.5,
            'keep_ratio': True
        }, {
            'type': 'Resize',
            'scale_factor': 0.75,
            'keep_ratio': True
        }, {
            'type': 'Resize',
            'scale_factor': 1.0,
            'keep_ratio': True
        }, {
            'type': 'Resize',
            'scale_factor': 1.25,
            'keep_ratio': True
        }, {
            'type': 'Resize',
            'scale_factor': 1.5,
            'keep_ratio': True
        }, {
            'type': 'Resize',
            'scale_factor': 1.75,
            'keep_ratio': True
        }],
                    [{
                        'type': 'RandomFlip',
                        'prob': 0.0,
                        'direction': 'horizontal'
                    }, {
                        'type': 'RandomFlip',
                        'prob': 1.0,
                        'direction': 'horizontal'
                    }], [{
                        'type': 'LoadAnnotations'
                    }], [{
                        'type': 'PackSegInputs'
                    }]])
]
train_dataloader = dict(
    batch_size=2,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(type='InfiniteSampler', shuffle=True),
    dataset=dict(
        type='ISICTask1Dataset',
        data_root='/content/dataset',
        data_prefix=dict(
            img_path='img_dir/ISIC2018_Task1-2_Training_Input',
            seg_map_path='ann_dir/ISIC2018_Task1_Training_GroundTruth'),
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),
            dict(type='RandomFlip', prob=0.5),
            dict(type='PackSegInputs')
        ]))
val_dataloader = dict(
    batch_size=1,
    num_workers=1,
    persistent_workers=True,
    sampler=dict(type='DefaultSampler', shuffle=False),
    dataset=dict(
        type='ISICTask1Dataset',
        data_root='/content/dataset',
        data_prefix=dict(
            img_path='img_dir/ISIC2018_Task1-2_Validation_Input',
            seg_map_path='ann_dir/ISIC2018_Task1_Validation_GroundTruth'),
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(type='PackSegInputs')
        ]))
test_dataloader = dict(
    batch_size=1,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(type='DefaultSampler', shuffle=False),
    dataset=dict(
        type='ISICTask1Dataset',
        data_root='/content/dataset',
        data_prefix=dict(
            img_path='img_dir/ISIC2018_Task1-2_Test_Input',
            seg_map_path='ann_dir/ISIC2018_Task1_Test_GroundTruth'),
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(type='PackSegInputs')
        ]))
val_evaluator = dict(type='IoUMetric', iou_metrics=['mIoU'])
test_evaluator = dict(type='IoUMetric', iou_metrics=['mIoU'])
custom_imports = dict(imports='mmdet.models', allow_failed_imports=False)
data_preprocessor = dict(
    type='SegDataPreProcessor',
    mean=[123.675, 116.28, 103.53],
    std=[58.395, 57.12, 57.375],
    bgr_to_rgb=True,
    pad_val=0,
    seg_pad_val=255,
    size=(512, 512),
    test_cfg=dict(size_divisor=32))
num_classes = 2
model = dict(
    type='EncoderDecoder',
    data_preprocessor=dict(
        type='SegDataPreProcessor',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        bgr_to_rgb=True,
        pad_val=0,
        seg_pad_val=255,
        size=(512, 512),
        test_cfg=dict(size_divisor=32)),
    backbone=dict(
        type='ResNet',
        depth=50,
        deep_stem=False,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=-1,
        norm_cfg=dict(type='SyncBN', requires_grad=False),
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
    decode_head=dict(
        type='Mask2FormerHead',
        in_channels=[256, 512, 1024, 2048],
        strides=[4, 8, 16, 32],
        feat_channels=256,
        out_channels=256,
        num_classes=150,
        num_queries=100,
        num_transformer_feat_level=3,
        align_corners=False,
        pixel_decoder=dict(
            type='mmdet.MSDeformAttnPixelDecoder',
            num_outs=3,
            norm_cfg=dict(type='GN', num_groups=32),
            act_cfg=dict(type='ReLU'),
            encoder=dict(
                num_layers=6,
                layer_cfg=dict(
                    self_attn_cfg=dict(
                        embed_dims=256,
                        num_heads=8,
                        num_levels=3,
                        num_points=4,
                        im2col_step=64,
                        dropout=0.0,
                        batch_first=True,
                        norm_cfg=None,
                        init_cfg=None),
                    ffn_cfg=dict(
                        embed_dims=256,
                        feedforward_channels=1024,
                        num_fcs=2,
                        ffn_drop=0.0,
                        act_cfg=dict(type='ReLU', inplace=True))),
                init_cfg=None),
            positional_encoding=dict(num_feats=128, normalize=True),
            init_cfg=None),
        enforce_decoder_input_project=False,
        positional_encoding=dict(num_feats=128, normalize=True),
        transformer_decoder=dict(
            return_intermediate=True,
            num_layers=9,
            layer_cfg=dict(
                self_attn_cfg=dict(
                    embed_dims=256,
                    num_heads=8,
                    attn_drop=0.0,
                    proj_drop=0.0,
                    dropout_layer=None,
                    batch_first=True),
                cross_attn_cfg=dict(
                    embed_dims=256,
                    num_heads=8,
                    attn_drop=0.0,
                    proj_drop=0.0,
                    dropout_layer=None,
                    batch_first=True),
                ffn_cfg=dict(
                    embed_dims=256,
                    feedforward_channels=2048,
                    num_fcs=2,
                    act_cfg=dict(type='ReLU', inplace=True),
                    ffn_drop=0.0,
                    dropout_layer=None,
                    add_identity=True)),
            init_cfg=None),
        loss_cls=dict(
            type='mmdet.CrossEntropyLoss',
            use_sigmoid=False,
            loss_weight=2.0,
            reduction='mean',
            class_weight=[
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1
            ]),
        loss_mask=dict(
            type='mmdet.CrossEntropyLoss',
            use_sigmoid=True,
            reduction='mean',
            loss_weight=5.0),
        loss_dice=dict(
            type='mmdet.DiceLoss',
            use_sigmoid=True,
            activate=True,
            reduction='mean',
            naive_dice=True,
            eps=1.0,
            loss_weight=5.0),
        train_cfg=dict(
            num_points=12544,
            oversample_ratio=3.0,
            importance_sample_ratio=0.75,
            assigner=dict(
                type='mmdet.HungarianAssigner',
                match_costs=[
                    dict(type='mmdet.ClassificationCost', weight=2.0),
                    dict(
                        type='mmdet.CrossEntropyLossCost',
                        weight=5.0,
                        use_sigmoid=True),
                    dict(
                        type='mmdet.DiceCost',
                        weight=5.0,
                        pred_act=True,
                        eps=1.0)
                ]),
            sampler=dict(type='mmdet.MaskPseudoSampler'))),
    train_cfg=dict(),
    test_cfg=dict(mode='whole'))
embed_multi = dict(lr_mult=1.0, decay_mult=0.0)
optimizer = dict(
    type='AdamW', lr=0.0001, weight_decay=0.05, eps=1e-08, betas=(0.9, 0.999))
optim_wrapper = dict(
    type='OptimWrapper',
    optimizer=dict(
        type='AdamW',
        lr=0.0001,
        weight_decay=0.05,
        eps=1e-08,
        betas=(0.9, 0.999)),
    clip_grad=dict(max_norm=0.01, norm_type=2),
    paramwise_cfg=dict(
        custom_keys=dict(
            backbone=dict(lr_mult=0.1, decay_mult=1.0),
            query_embed=dict(lr_mult=1.0, decay_mult=0.0),
            query_feat=dict(lr_mult=1.0, decay_mult=0.0),
            level_embed=dict(lr_mult=1.0, decay_mult=0.0)),
        norm_decay_mult=0.0))
param_scheduler = [
    dict(type='LinearLR', by_epoch=False, start_factor=0.1, begin=0, end=100),
    dict(
        type='PolyLR',
        eta_min=0.0001,
        power=0.9,
        begin=0,
        end=160,
        by_epoch=False)
]
train_cfg = dict(
    type='EpochBasedTrainLoop', max_epochs=5, val_begin=1, val_interval=1000)
val_cfg = dict(type='ValLoop')
test_cfg = dict(type='TestLoop')
default_hooks = dict(
    timer=dict(type='IterTimerHook'),
    logger=dict(type='LoggerHook', interval=50, log_metric_by_epoch=False),
    param_scheduler=dict(type='ParamSchedulerHook'),
    checkpoint=dict(type='CheckpointHook', interval=250, by_epoch=False),
    sampler_seed=dict(type='DistSamplerSeedHook'))
auto_scale_lr = dict(enable=False, base_batch_size=16)
work_dir = './work_dirs/tutorial'
randomness = dict(seed=48)

2023/05/23 07:07:28 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
2023/05/23 07:07:28 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train:
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
2023/05/23 07:07:30 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:lr=1e-05
2023/05/23 07:07:30 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:weight_decay=0.05
2023/05/23 07:07:30 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:lr_mult=0.1
2023/05/23 07:07:30 - mmengine - INFO - paramwise_options -- backbone.conv1.weight:decay_mult=1.0
2023/05/23 07:07:30 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:lr=1e-05
2023/05/23 07:07:30 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:weight_decay=0.05
2023/05/23 07:07:30 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv1.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv2.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.0.conv3.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.0.downsample.0.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv1.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv2.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.1.conv3.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv1.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv2.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer1.2.conv3.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv1.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv2.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.0.conv3.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.0.downsample.0.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv1.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv2.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.1.conv3.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv1.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv2.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.2.conv3.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv1.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv2.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer2.3.conv3.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv1.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv2.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.0.conv3.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.0.downsample.0.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv1.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv2.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.1.conv3.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv1.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv2.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.2.conv3.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv1.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv2.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.3.conv3.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv1.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv2.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.4.conv3.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv1.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv2.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer3.5.conv3.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv1.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv2.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.0.conv3.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.0.downsample.0.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv1.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv2.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.1.conv3.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv1.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv2.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:lr=1e-05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:weight_decay=0.05
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:lr_mult=0.1
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- backbone.layer4.2.conv3.weight:decay_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.0.gn.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.0.gn.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.1.gn.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.1.gn.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.2.gn.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.2.gn.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.0.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.0.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.1.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.1.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.0.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.0.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.1.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.1.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.0.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.0.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.1.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.1.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.0.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.0.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.1.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.1.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.0.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.0.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.1.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.1.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.0.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.0.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.1.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.1.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.lateral_convs.0.gn.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.lateral_convs.0.gn.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.output_convs.0.gn.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.output_convs.0.gn.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.0.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.0.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.1.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.1.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.2.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.2.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.0.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.0.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.1.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.1.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.2.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.2.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.0.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.0.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.1.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.1.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.2.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.2.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.0.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.0.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.1.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.1.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.2.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.2.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.0.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.0.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.1.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.1.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.2.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.2.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.0.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.0.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.1.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.1.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.2.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.2.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.0.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.0.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.1.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.1.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.2.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.2.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.0.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.0.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.1.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.1.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.2.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.2.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.0.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.0.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.1.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.1.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.2.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.2.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.post_norm.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.post_norm.bias:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:lr=0.0001
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:lr_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:decay_mult=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.query_feat.weight:lr=0.0001
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.query_feat.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.query_feat.weight:lr_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.query_feat.weight:decay_mult=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:lr=0.0001
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:weight_decay=0.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:lr_mult=1.0
2023/05/23 07:07:31 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:decay_mult=0.0
2023/05/23 07:07:32 - mmengine - WARNING - The prefix is not set in metric class IoUMetric.
2023/05/23 07:07:34 - mmengine - INFO - load model from: torchvision://resnet50
2023/05/23 07:07:34 - mmengine - INFO - Loads checkpoint by torchvision backend from path: torchvision://resnet50
2023/05/23 07:07:35 - mmengine - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: fc.weight, fc.bias

Name of parameter - Initialization information

backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
PretrainedInit: load from torchvision://resnet50 

backbone.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

decode_head.pixel_decoder.input_convs.0.conv.weight - torch.Size([256, 2048, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.input_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.input_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.input_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.input_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.input_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.input_convs.1.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.input_convs.1.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.input_convs.2.conv.weight - torch.Size([256, 512, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.input_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.input_convs.2.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.input_convs.2.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.0.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.0.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.0.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.0.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.0.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.0.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.0.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.1.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.1.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.1.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.1.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.1.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.1.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.1.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.2.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.2.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.2.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.2.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.2.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.2.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.2.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.3.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.3.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.3.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.3.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.3.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.3.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.3.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.4.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.4.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.4.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.4.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.4.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.4.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.4.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.5.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.5.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.5.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.5.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.5.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.5.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.5.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.level_encoding.weight - torch.Size([3, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.lateral_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.lateral_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.output_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.output_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.output_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.mask_feature.weight - torch.Size([256, 256, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.mask_feature.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.0.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.0.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.0.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.0.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.0.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.1.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.1.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.1.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.1.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.1.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.1.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.2.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.2.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.2.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.2.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.2.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.2.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.3.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.3.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.3.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.3.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.3.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.3.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.4.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.4.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.4.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.4.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.4.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.4.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.5.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.5.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.5.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.5.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.5.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.5.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.6.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.6.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.6.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.6.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.6.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.6.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.6.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.6.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.6.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.6.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.6.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.6.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.7.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.7.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.7.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.7.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.7.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.7.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.7.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.7.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.7.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.7.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.7.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.7.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.8.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.8.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.8.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.8.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.8.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.8.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.8.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.8.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.8.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.8.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.8.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.8.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.8.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.8.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.8.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.8.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.8.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.8.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.post_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.post_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.query_embed.weight - torch.Size([100, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.query_feat.weight - torch.Size([100, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.level_embed.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.cls_embed.weight - torch.Size([151, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.cls_embed.bias - torch.Size([151]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.mask_embed.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.mask_embed.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.mask_embed.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.mask_embed.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.mask_embed.4.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.mask_embed.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  
2023/05/23 07:07:54 - mmengine - INFO - Load checkpoint from https://download.openmmlab.com/mmsegmentation/v0.5/mask2former/mask2former_r50_8xb2-160k_ade20k-512x512/mask2former_r50_8xb2-160k_ade20k-512x512_20221204_000055-2d1f55f1.pth
2023/05/23 07:07:54 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
2023/05/23 07:07:54 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
2023/05/23 07:07:54 - mmengine - INFO - Checkpoints will be saved to /content/mmsegmentation/work_dirs/tutorial.
2023/05/23 07:08:46 - mmengine - INFO - Epoch(train) [1][  50/1297]  lr: 5.6271e-05  eta: 1:51:29  time: 0.6882  data_time: 0.0120  memory: 10471  grad_norm: 907.7501  loss: 36.0751  decode.loss_cls: 0.9636  decode.loss_mask: 1.6424  decode.loss_dice: 1.0209  decode.d0.loss_cls: 1.4742  decode.d0.loss_mask: 1.7369  decode.d0.loss_dice: 1.0702  decode.d1.loss_cls: 0.6629  decode.d1.loss_mask: 1.4595  decode.d1.loss_dice: 0.9697  decode.d2.loss_cls: 0.6648  decode.d2.loss_mask: 2.2257  decode.d2.loss_dice: 1.1070  decode.d3.loss_cls: 0.5160  decode.d3.loss_mask: 2.2019  decode.d3.loss_dice: 1.0925  decode.d4.loss_cls: 0.7394  decode.d4.loss_mask: 1.2833  decode.d4.loss_dice: 0.9406  decode.d5.loss_cls: 0.7336  decode.d5.loss_mask: 1.3560  decode.d5.loss_dice: 0.8937  decode.d6.loss_cls: 0.8252  decode.d6.loss_mask: 1.8204  decode.d6.loss_dice: 1.1949  decode.d7.loss_cls: 0.8796  decode.d7.loss_mask: 1.9420  decode.d7.loss_dice: 1.1322  decode.d8.loss_cls: 0.9117  decode.d8.loss_mask: 1.5963  decode.d8.loss_dice: 1.0179
2023/05/23 07:09:20 - mmengine - INFO - Epoch(train) [1][ 100/1297]  lr: 1.1245e-04  eta: 1:31:17  time: 0.6115  data_time: 0.0076  memory: 2830  grad_norm: 935.9803  loss: 32.3834  decode.loss_cls: 0.4038  decode.loss_mask: 1.5015  decode.loss_dice: 0.6716  decode.d0.loss_cls: 1.0525  decode.d0.loss_mask: 0.8551  decode.d0.loss_dice: 0.5921  decode.d1.loss_cls: 0.8692  decode.d1.loss_mask: 1.3708  decode.d1.loss_dice: 0.8416  decode.d2.loss_cls: 0.6938  decode.d2.loss_mask: 2.0040  decode.d2.loss_dice: 0.6537  decode.d3.loss_cls: 0.6667  decode.d3.loss_mask: 2.3546  decode.d3.loss_dice: 0.8005  decode.d4.loss_cls: 0.5758  decode.d4.loss_mask: 3.6092  decode.d4.loss_dice: 0.8967  decode.d5.loss_cls: 0.6495  decode.d5.loss_mask: 2.1776  decode.d5.loss_dice: 0.7983  decode.d6.loss_cls: 0.5501  decode.d6.loss_mask: 1.9284  decode.d6.loss_dice: 0.7959  decode.d7.loss_cls: 0.5385  decode.d7.loss_mask: 1.3358  decode.d7.loss_dice: 0.5922  decode.d8.loss_cls: 0.4870  decode.d8.loss_mask: 1.4710  decode.d8.loss_dice: 0.6459
2023/05/23 07:09:57 - mmengine - INFO - Epoch(train) [1][ 150/1297]  lr: 1.0248e-04  eta: 1:26:04  time: 0.6670  data_time: 0.0101  memory: 2839  grad_norm: 491.7650  loss: 27.0897  decode.loss_cls: 0.3402  decode.loss_mask: 1.3996  decode.loss_dice: 0.9173  decode.d0.loss_cls: 0.5612  decode.d0.loss_mask: 1.2181  decode.d0.loss_dice: 0.8523  decode.d1.loss_cls: 0.6473  decode.d1.loss_mask: 1.4287  decode.d1.loss_dice: 0.8217  decode.d2.loss_cls: 0.5250  decode.d2.loss_mask: 1.3052  decode.d2.loss_dice: 0.7255  decode.d3.loss_cls: 0.6154  decode.d3.loss_mask: 1.3296  decode.d3.loss_dice: 0.6626  decode.d4.loss_cls: 0.4887  decode.d4.loss_mask: 1.2931  decode.d4.loss_dice: 0.7694  decode.d5.loss_cls: 0.3620  decode.d5.loss_mask: 1.6582  decode.d5.loss_dice: 0.9293  decode.d6.loss_cls: 0.3011  decode.d6.loss_mask: 1.5709  decode.d6.loss_dice: 0.9430  decode.d7.loss_cls: 0.3486  decode.d7.loss_mask: 1.4803  decode.d7.loss_dice: 0.9645  decode.d8.loss_cls: 0.3375  decode.d8.loss_mask: 1.3553  decode.d8.loss_dice: 0.9378
2023/05/23 07:10:35 - mmengine - INFO - Epoch(train) [1][ 200/1297]  lr: 1.0000e-04  eta: 1:24:11  time: 0.9069  data_time: 0.0321  memory: 2829  grad_norm: 1068.3649  loss: 21.6962  decode.loss_cls: 0.4382  decode.loss_mask: 1.3671  decode.loss_dice: 0.7406  decode.d0.loss_cls: 0.6590  decode.d0.loss_mask: 1.1011  decode.d0.loss_dice: 0.5186  decode.d1.loss_cls: 0.2555  decode.d1.loss_mask: 0.9694  decode.d1.loss_dice: 0.4634  decode.d2.loss_cls: 0.3820  decode.d2.loss_mask: 0.8949  decode.d2.loss_dice: 0.4856  decode.d3.loss_cls: 0.3387  decode.d3.loss_mask: 1.0532  decode.d3.loss_dice: 0.4988  decode.d4.loss_cls: 0.3720  decode.d4.loss_mask: 1.0725  decode.d4.loss_dice: 0.4709  decode.d5.loss_cls: 0.4549  decode.d5.loss_mask: 0.7891  decode.d5.loss_dice: 0.4577  decode.d6.loss_cls: 0.4450  decode.d6.loss_mask: 1.4494  decode.d6.loss_dice: 0.6464  decode.d7.loss_cls: 0.4288  decode.d7.loss_mask: 1.5973  decode.d7.loss_dice: 0.7474  decode.d8.loss_cls: 0.5353  decode.d8.loss_mask: 1.3757  decode.d8.loss_dice: 0.6879
2023/05/23 07:11:09 - mmengine - INFO - Epoch(train) [1][ 250/1297]  lr: 1.0000e-04  eta: 1:21:02  time: 0.8185  data_time: 0.0493  memory: 2829  grad_norm: 374.1369  loss: 23.9695  decode.loss_cls: 0.3674  decode.loss_mask: 1.3426  decode.loss_dice: 0.6114  decode.d0.loss_cls: 0.7655  decode.d0.loss_mask: 1.0996  decode.d0.loss_dice: 0.5535  decode.d1.loss_cls: 0.3279  decode.d1.loss_mask: 1.4233  decode.d1.loss_dice: 0.6873  decode.d2.loss_cls: 0.3467  decode.d2.loss_mask: 1.4749  decode.d2.loss_dice: 0.6048  decode.d3.loss_cls: 0.3547  decode.d3.loss_mask: 1.4811  decode.d3.loss_dice: 0.7069  decode.d4.loss_cls: 0.2914  decode.d4.loss_mask: 1.5445  decode.d4.loss_dice: 0.7794  decode.d5.loss_cls: 0.2989  decode.d5.loss_mask: 1.4361  decode.d5.loss_dice: 0.5847  decode.d6.loss_cls: 0.3908  decode.d6.loss_mask: 1.3202  decode.d6.loss_dice: 0.5282  decode.d7.loss_cls: 0.3859  decode.d7.loss_mask: 1.3671  decode.d7.loss_dice: 0.5272  decode.d8.loss_cls: 0.3175  decode.d8.loss_mask: 1.4625  decode.d8.loss_dice: 0.5874
2023/05/23 07:11:09 - mmengine - INFO - Saving checkpoint at 250 iterations
2023/05/23 07:11:52 - mmengine - INFO - Epoch(train) [1][ 300/1297]  lr: 1.0000e-04  eta: 1:21:44  time: 0.8086  data_time: 0.0529  memory: 2829  grad_norm: 518.8786  loss: 19.6462  decode.loss_cls: 0.3938  decode.loss_mask: 0.9652  decode.loss_dice: 0.5019  decode.d0.loss_cls: 0.4579  decode.d0.loss_mask: 1.0741  decode.d0.loss_dice: 0.4503  decode.d1.loss_cls: 0.7025  decode.d1.loss_mask: 0.8648  decode.d1.loss_dice: 0.5403  decode.d2.loss_cls: 0.3933  decode.d2.loss_mask: 1.2211  decode.d2.loss_dice: 0.5622  decode.d3.loss_cls: 0.3468  decode.d3.loss_mask: 0.9318  decode.d3.loss_dice: 0.5056  decode.d4.loss_cls: 0.3535  decode.d4.loss_mask: 1.1063  decode.d4.loss_dice: 0.4978  decode.d5.loss_cls: 0.3450  decode.d5.loss_mask: 0.9669  decode.d5.loss_dice: 0.4799  decode.d6.loss_cls: 0.4049  decode.d6.loss_mask: 0.9934  decode.d6.loss_dice: 0.4786  decode.d7.loss_cls: 0.3761  decode.d7.loss_mask: 1.0615  decode.d7.loss_dice: 0.5905  decode.d8.loss_cls: 0.3990  decode.d8.loss_mask: 1.0695  decode.d8.loss_dice: 0.6119
2023/05/23 07:12:31 - mmengine - INFO - Epoch(train) [1][ 350/1297]  lr: 1.0000e-04  eta: 1:20:43  time: 0.7729  data_time: 0.0139  memory: 2829  grad_norm: 223.4269  loss: 20.4918  decode.loss_cls: 0.2212  decode.loss_mask: 1.0397  decode.loss_dice: 0.7299  decode.d0.loss_cls: 0.4200  decode.d0.loss_mask: 1.0360  decode.d0.loss_dice: 0.7618  decode.d1.loss_cls: 0.2612  decode.d1.loss_mask: 1.0666  decode.d1.loss_dice: 0.7009  decode.d2.loss_cls: 0.2414  decode.d2.loss_mask: 1.0351  decode.d2.loss_dice: 0.6958  decode.d3.loss_cls: 0.1918  decode.d3.loss_mask: 1.0757  decode.d3.loss_dice: 0.7374  decode.d4.loss_cls: 0.2058  decode.d4.loss_mask: 1.1488  decode.d4.loss_dice: 0.7519  decode.d5.loss_cls: 0.1870  decode.d5.loss_mask: 1.1327  decode.d5.loss_dice: 0.7534  decode.d6.loss_cls: 0.2360  decode.d6.loss_mask: 1.1075  decode.d6.loss_dice: 0.7799  decode.d7.loss_cls: 0.2295  decode.d7.loss_mask: 1.0219  decode.d7.loss_dice: 0.7373  decode.d8.loss_cls: 0.2179  decode.d8.loss_mask: 1.0383  decode.d8.loss_dice: 0.7293
2023/05/23 07:13:06 - mmengine - INFO - Epoch(train) [1][ 400/1297]  lr: 1.0000e-04  eta: 1:18:58  time: 0.6045  data_time: 0.0073  memory: 2829  grad_norm: 384.0076  loss: 25.9073  decode.loss_cls: 0.5517  decode.loss_mask: 1.0797  decode.loss_dice: 1.0229  decode.d0.loss_cls: 0.5485  decode.d0.loss_mask: 1.0394  decode.d0.loss_dice: 0.7535  decode.d1.loss_cls: 0.4518  decode.d1.loss_mask: 1.4537  decode.d1.loss_dice: 1.1038  decode.d2.loss_cls: 0.5201  decode.d2.loss_mask: 0.8869  decode.d2.loss_dice: 0.7722  decode.d3.loss_cls: 0.5298  decode.d3.loss_mask: 0.9076  decode.d3.loss_dice: 0.8651  decode.d4.loss_cls: 0.4770  decode.d4.loss_mask: 1.0648  decode.d4.loss_dice: 0.9730  decode.d5.loss_cls: 0.3735  decode.d5.loss_mask: 1.2340  decode.d5.loss_dice: 1.1451  decode.d6.loss_cls: 0.3723  decode.d6.loss_mask: 1.2303  decode.d6.loss_dice: 1.1395  decode.d7.loss_cls: 0.4469  decode.d7.loss_mask: 1.2880  decode.d7.loss_dice: 1.1412  decode.d8.loss_cls: 0.4093  decode.d8.loss_mask: 1.0887  decode.d8.loss_dice: 1.0370
2023/05/23 07:13:42 - mmengine - INFO - Epoch(train) [1][ 450/1297]  lr: 1.0000e-04  eta: 1:17:39  time: 0.7261  data_time: 0.0125  memory: 2829  grad_norm: 367.5263  loss: 25.8895  decode.loss_cls: 0.2287  decode.loss_mask: 1.3746  decode.loss_dice: 0.5582  decode.d0.loss_cls: 0.4094  decode.d0.loss_mask: 1.1359  decode.d0.loss_dice: 0.4782  decode.d1.loss_cls: 0.3759  decode.d1.loss_mask: 1.2146  decode.d1.loss_dice: 0.4878  decode.d2.loss_cls: 0.1734  decode.d2.loss_mask: 2.2652  decode.d2.loss_dice: 0.5456  decode.d3.loss_cls: 0.3007  decode.d3.loss_mask: 1.3710  decode.d3.loss_dice: 0.5391  decode.d4.loss_cls: 0.2913  decode.d4.loss_mask: 1.3668  decode.d4.loss_dice: 0.5210  decode.d5.loss_cls: 0.2504  decode.d5.loss_mask: 1.5506  decode.d5.loss_dice: 0.5609  decode.d6.loss_cls: 0.1909  decode.d6.loss_mask: 3.3626  decode.d6.loss_dice: 0.6259  decode.d7.loss_cls: 0.2135  decode.d7.loss_mask: 2.2518  decode.d7.loss_dice: 0.6210  decode.d8.loss_cls: 0.1984  decode.d8.loss_mask: 1.8196  decode.d8.loss_dice: 0.6065
2023/05/23 07:14:16 - mmengine - INFO - Epoch(train) [1][ 500/1297]  lr: 1.0000e-04  eta: 1:16:05  time: 0.8021  data_time: 0.0152  memory: 2830  grad_norm: 526.0481  loss: 26.7442  decode.loss_cls: 0.5652  decode.loss_mask: 0.9933  decode.loss_dice: 0.6115  decode.d0.loss_cls: 0.5876  decode.d0.loss_mask: 0.7710  decode.d0.loss_dice: 0.4103  decode.d1.loss_cls: 0.6599  decode.d1.loss_mask: 1.3573  decode.d1.loss_dice: 0.6351  decode.d2.loss_cls: 0.5937  decode.d2.loss_mask: 2.0771  decode.d2.loss_dice: 0.7158  decode.d3.loss_cls: 0.5498  decode.d3.loss_mask: 2.0419  decode.d3.loss_dice: 0.6822  decode.d4.loss_cls: 0.5974  decode.d4.loss_mask: 1.8966  decode.d4.loss_dice: 0.5096  decode.d5.loss_cls: 0.5567  decode.d5.loss_mask: 0.9766  decode.d5.loss_dice: 0.4555  decode.d6.loss_cls: 0.5776  decode.d6.loss_mask: 1.7617  decode.d6.loss_dice: 0.6699  decode.d7.loss_cls: 0.4379  decode.d7.loss_mask: 2.0071  decode.d7.loss_dice: 0.7365  decode.d8.loss_cls: 0.4343  decode.d8.loss_mask: 1.2170  decode.d8.loss_dice: 0.6582
2023/05/23 07:14:16 - mmengine - INFO - Saving checkpoint at 500 iterations
2023/05/23 07:14:55 - mmengine - INFO - Epoch(train) [1][ 550/1297]  lr: 1.0000e-04  eta: 1:15:40  time: 0.6242  data_time: 0.0117  memory: 2830  grad_norm: 164.3937  loss: 15.8304  decode.loss_cls: 0.2280  decode.loss_mask: 0.9055  decode.loss_dice: 0.5825  decode.d0.loss_cls: 0.3387  decode.d0.loss_mask: 0.9088  decode.d0.loss_dice: 0.4527  decode.d1.loss_cls: 0.1158  decode.d1.loss_mask: 0.9306  decode.d1.loss_dice: 0.4243  decode.d2.loss_cls: 0.1121  decode.d2.loss_mask: 0.9160  decode.d2.loss_dice: 0.4226  decode.d3.loss_cls: 0.2188  decode.d3.loss_mask: 0.8756  decode.d3.loss_dice: 0.4520  decode.d4.loss_cls: 0.1300  decode.d4.loss_mask: 0.9333  decode.d4.loss_dice: 0.4511  decode.d5.loss_cls: 0.0879  decode.d5.loss_mask: 0.9953  decode.d5.loss_dice: 0.4479  decode.d6.loss_cls: 0.1179  decode.d6.loss_mask: 1.0511  decode.d6.loss_dice: 0.4638  decode.d7.loss_cls: 0.0947  decode.d7.loss_mask: 1.0907  decode.d7.loss_dice: 0.5117  decode.d8.loss_cls: 0.2265  decode.d8.loss_mask: 0.8960  decode.d8.loss_dice: 0.4488
2023/05/23 07:15:31 - mmengine - INFO - Epoch(train) [1][ 600/1297]  lr: 1.0000e-04  eta: 1:14:39  time: 0.6543  data_time: 0.0199  memory: 2846  grad_norm: 129.1504  loss: 11.6109  decode.loss_cls: 0.1551  decode.loss_mask: 0.6311  decode.loss_dice: 0.2769  decode.d0.loss_cls: 0.1843  decode.d0.loss_mask: 0.7253  decode.d0.loss_dice: 0.3403  decode.d1.loss_cls: 0.3508  decode.d1.loss_mask: 0.5323  decode.d1.loss_dice: 0.2118  decode.d2.loss_cls: 0.3302  decode.d2.loss_mask: 0.5167  decode.d2.loss_dice: 0.2522  decode.d3.loss_cls: 0.3109  decode.d3.loss_mask: 0.6882  decode.d3.loss_dice: 0.3551  decode.d4.loss_cls: 0.1689  decode.d4.loss_mask: 0.7540  decode.d4.loss_dice: 0.3056  decode.d5.loss_cls: 0.1408  decode.d5.loss_mask: 0.7162  decode.d5.loss_dice: 0.3019  decode.d6.loss_cls: 0.1605  decode.d6.loss_mask: 0.7032  decode.d6.loss_dice: 0.2959  decode.d7.loss_cls: 0.1931  decode.d7.loss_mask: 0.5756  decode.d7.loss_dice: 0.2880  decode.d8.loss_cls: 0.1807  decode.d8.loss_mask: 0.6845  decode.d8.loss_dice: 0.2811
2023/05/23 07:16:10 - mmengine - INFO - Epoch(train) [1][ 650/1297]  lr: 1.0000e-04  eta: 1:14:06  time: 0.8112  data_time: 0.0180  memory: 2830  grad_norm: 415.6732  loss: 20.1703  decode.loss_cls: 0.3405  decode.loss_mask: 1.0657  decode.loss_dice: 0.4576  decode.d0.loss_cls: 0.3635  decode.d0.loss_mask: 1.1781  decode.d0.loss_dice: 0.3995  decode.d1.loss_cls: 0.4022  decode.d1.loss_mask: 1.0255  decode.d1.loss_dice: 0.4324  decode.d2.loss_cls: 0.3629  decode.d2.loss_mask: 1.3230  decode.d2.loss_dice: 0.6037  decode.d3.loss_cls: 0.2494  decode.d3.loss_mask: 1.4243  decode.d3.loss_dice: 0.5906  decode.d4.loss_cls: 0.3376  decode.d4.loss_mask: 1.2704  decode.d4.loss_dice: 0.4975  decode.d5.loss_cls: 0.3602  decode.d5.loss_mask: 1.0974  decode.d5.loss_dice: 0.5194  decode.d6.loss_cls: 0.3849  decode.d6.loss_mask: 1.1219  decode.d6.loss_dice: 0.4927  decode.d7.loss_cls: 0.3469  decode.d7.loss_mask: 1.0927  decode.d7.loss_dice: 0.4460  decode.d8.loss_cls: 0.3127  decode.d8.loss_mask: 1.1893  decode.d8.loss_dice: 0.4818
2023/05/23 07:16:45 - mmengine - INFO - Epoch(train) [1][ 700/1297]  lr: 1.0000e-04  eta: 1:13:00  time: 0.6891  data_time: 0.0098  memory: 2829  grad_norm: 373.1437  loss: 30.0668  decode.loss_cls: 0.4781  decode.loss_mask: 1.5842  decode.loss_dice: 0.7223  decode.d0.loss_cls: 0.3939  decode.d0.loss_mask: 1.3987  decode.d0.loss_dice: 0.7520  decode.d1.loss_cls: 0.4272  decode.d1.loss_mask: 2.0889  decode.d1.loss_dice: 0.7947  decode.d2.loss_cls: 0.4923  decode.d2.loss_mask: 1.5454  decode.d2.loss_dice: 0.9317  decode.d3.loss_cls: 0.3675  decode.d3.loss_mask: 2.6280  decode.d3.loss_dice: 0.7333  decode.d4.loss_cls: 0.3473  decode.d4.loss_mask: 2.2719  decode.d4.loss_dice: 0.8302  decode.d5.loss_cls: 0.3710  decode.d5.loss_mask: 1.6574  decode.d5.loss_dice: 0.7984  decode.d6.loss_cls: 0.4724  decode.d6.loss_mask: 1.6021  decode.d6.loss_dice: 0.7213  decode.d7.loss_cls: 0.5180  decode.d7.loss_mask: 1.5170  decode.d7.loss_dice: 0.7322  decode.d8.loss_cls: 0.4799  decode.d8.loss_mask: 1.6809  decode.d8.loss_dice: 0.7285
2023/05/23 07:17:20 - mmengine - INFO - Epoch(train) [1][ 750/1297]  lr: 1.0000e-04  eta: 1:12:07  time: 0.6717  data_time: 0.0088  memory: 2829  grad_norm: 366.0380  loss: 23.7038  decode.loss_cls: 0.3546  decode.loss_mask: 1.0984  decode.loss_dice: 0.7368  decode.d0.loss_cls: 0.5728  decode.d0.loss_mask: 1.0910  decode.d0.loss_dice: 0.8054  decode.d1.loss_cls: 0.3324  decode.d1.loss_mask: 1.1594  decode.d1.loss_dice: 0.9193  decode.d2.loss_cls: 0.3190  decode.d2.loss_mask: 1.0156  decode.d2.loss_dice: 1.1521  decode.d3.loss_cls: 0.2438  decode.d3.loss_mask: 1.3240  decode.d3.loss_dice: 1.0917  decode.d4.loss_cls: 0.2448  decode.d4.loss_mask: 1.1915  decode.d4.loss_dice: 0.9494  decode.d5.loss_cls: 0.3130  decode.d5.loss_mask: 1.0195  decode.d5.loss_dice: 0.8242  decode.d6.loss_cls: 0.3352  decode.d6.loss_mask: 1.2117  decode.d6.loss_dice: 0.9395  decode.d7.loss_cls: 0.3172  decode.d7.loss_mask: 0.9694  decode.d7.loss_dice: 0.8228  decode.d8.loss_cls: 0.2577  decode.d8.loss_mask: 1.2211  decode.d8.loss_dice: 0.8704
2023/05/23 07:17:20 - mmengine - INFO - Saving checkpoint at 750 iterations
2023/05/23 07:18:04 - mmengine - INFO - Epoch(train) [1][ 800/1297]  lr: 1.0000e-04  eta: 1:12:10  time: 0.7775  data_time: 0.0404  memory: 2829  grad_norm: 281.7647  loss: 22.1578  decode.loss_cls: 0.4564  decode.loss_mask: 0.6729  decode.loss_dice: 0.7592  decode.d0.loss_cls: 0.6375  decode.d0.loss_mask: 0.5982  decode.d0.loss_dice: 0.6595  decode.d1.loss_cls: 0.7160  decode.d1.loss_mask: 0.9035  decode.d1.loss_dice: 0.7038  decode.d2.loss_cls: 0.7790  decode.d2.loss_mask: 0.8075  decode.d2.loss_dice: 0.7469  decode.d3.loss_cls: 0.7008  decode.d3.loss_mask: 0.9428  decode.d3.loss_dice: 0.8419  decode.d4.loss_cls: 0.5684  decode.d4.loss_mask: 0.8825  decode.d4.loss_dice: 0.8410  decode.d5.loss_cls: 0.5226  decode.d5.loss_mask: 0.9351  decode.d5.loss_dice: 0.8275  decode.d6.loss_cls: 0.4718  decode.d6.loss_mask: 1.2414  decode.d6.loss_dice: 0.8262  decode.d7.loss_cls: 0.5537  decode.d7.loss_mask: 0.7228  decode.d7.loss_dice: 0.7375  decode.d8.loss_cls: 0.4957  decode.d8.loss_mask: 0.8217  decode.d8.loss_dice: 0.7842
2023/05/23 07:18:41 - mmengine - INFO - Epoch(train) [1][ 850/1297]  lr: 1.0000e-04  eta: 1:11:27  time: 0.7024  data_time: 0.0366  memory: 2829  grad_norm: 262.4040  loss: 22.4652  decode.loss_cls: 0.5361  decode.loss_mask: 1.1552  decode.loss_dice: 0.8624  decode.d0.loss_cls: 0.3589  decode.d0.loss_mask: 0.9389  decode.d0.loss_dice: 0.8149  decode.d1.loss_cls: 0.4894  decode.d1.loss_mask: 0.8670  decode.d1.loss_dice: 0.8024  decode.d2.loss_cls: 0.3904  decode.d2.loss_mask: 0.9282  decode.d2.loss_dice: 0.8170  decode.d3.loss_cls: 0.3541  decode.d3.loss_mask: 0.9926  decode.d3.loss_dice: 0.7336  decode.d4.loss_cls: 0.4213  decode.d4.loss_mask: 0.9493  decode.d4.loss_dice: 0.8019  decode.d5.loss_cls: 0.5171  decode.d5.loss_mask: 0.9666  decode.d5.loss_dice: 0.7596  decode.d6.loss_cls: 0.5279  decode.d6.loss_mask: 0.9795  decode.d6.loss_dice: 0.7848  decode.d7.loss_cls: 0.5550  decode.d7.loss_mask: 1.0112  decode.d7.loss_dice: 0.7617  decode.d8.loss_cls: 0.5396  decode.d8.loss_mask: 1.0185  decode.d8.loss_dice: 0.8302
2023/05/23 07:19:15 - mmengine - INFO - Epoch(train) [1][ 900/1297]  lr: 1.0000e-04  eta: 1:10:21  time: 0.6484  data_time: 0.0124  memory: 2829  grad_norm: 196.5307  loss: 13.5262  decode.loss_cls: 0.2591  decode.loss_mask: 0.5802  decode.loss_dice: 0.3734  decode.d0.loss_cls: 0.4732  decode.d0.loss_mask: 0.6694  decode.d0.loss_dice: 0.4703  decode.d1.loss_cls: 0.4189  decode.d1.loss_mask: 0.7416  decode.d1.loss_dice: 0.4317  decode.d2.loss_cls: 0.3244  decode.d2.loss_mask: 0.6077  decode.d2.loss_dice: 0.3886  decode.d3.loss_cls: 0.3741  decode.d3.loss_mask: 0.6576  decode.d3.loss_dice: 0.3916  decode.d4.loss_cls: 0.2992  decode.d4.loss_mask: 0.6389  decode.d4.loss_dice: 0.3871  decode.d5.loss_cls: 0.3606  decode.d5.loss_mask: 0.5658  decode.d5.loss_dice: 0.3404  decode.d6.loss_cls: 0.2667  decode.d6.loss_mask: 0.5918  decode.d6.loss_dice: 0.3423  decode.d7.loss_cls: 0.3496  decode.d7.loss_mask: 0.6016  decode.d7.loss_dice: 0.3388  decode.d8.loss_cls: 0.2484  decode.d8.loss_mask: 0.6113  decode.d8.loss_dice: 0.4219
2023/05/23 07:19:51 - mmengine - INFO - Epoch(train) [1][ 950/1297]  lr: 1.0000e-04  eta: 1:09:32  time: 0.8109  data_time: 0.0853  memory: 2829  grad_norm: 296.3195  loss: 16.6007  decode.loss_cls: 0.4459  decode.loss_mask: 0.7653  decode.loss_dice: 0.4322  decode.d0.loss_cls: 0.4089  decode.d0.loss_mask: 0.8975  decode.d0.loss_dice: 0.5433  decode.d1.loss_cls: 0.4220  decode.d1.loss_mask: 0.8030  decode.d1.loss_dice: 0.5436  decode.d2.loss_cls: 0.4500  decode.d2.loss_mask: 0.7614  decode.d2.loss_dice: 0.4774  decode.d3.loss_cls: 0.3995  decode.d3.loss_mask: 0.6989  decode.d3.loss_dice: 0.3902  decode.d4.loss_cls: 0.4352  decode.d4.loss_mask: 0.6824  decode.d4.loss_dice: 0.4941  decode.d5.loss_cls: 0.3887  decode.d5.loss_mask: 0.6877  decode.d5.loss_dice: 0.4123  decode.d6.loss_cls: 0.3545  decode.d6.loss_mask: 0.7835  decode.d6.loss_dice: 0.4252  decode.d7.loss_cls: 0.3829  decode.d7.loss_mask: 0.8095  decode.d7.loss_dice: 0.5100  decode.d8.loss_cls: 0.4643  decode.d8.loss_mask: 0.8435  decode.d8.loss_dice: 0.4878
2023/05/23 07:20:26 - mmengine - INFO - Exp name: mask2former_r50_8xb2-160k_ade20k-512x512_20230523_070719
2023/05/23 07:20:26 - mmengine - INFO - Epoch(train) [1][1000/1297]  lr: 1.0000e-04  eta: 1:08:44  time: 0.7587  data_time: 0.0113  memory: 2830  grad_norm: 200.0510  loss: 15.2708  decode.loss_cls: 0.2003  decode.loss_mask: 0.8023  decode.loss_dice: 0.5674  decode.d0.loss_cls: 0.0635  decode.d0.loss_mask: 0.7888  decode.d0.loss_dice: 0.6174  decode.d1.loss_cls: 0.1598  decode.d1.loss_mask: 0.7895  decode.d1.loss_dice: 0.5322  decode.d2.loss_cls: 0.1933  decode.d2.loss_mask: 0.7364  decode.d2.loss_dice: 0.5060  decode.d3.loss_cls: 0.1220  decode.d3.loss_mask: 0.8163  decode.d3.loss_dice: 0.5133  decode.d4.loss_cls: 0.0910  decode.d4.loss_mask: 0.8627  decode.d4.loss_dice: 0.5960  decode.d5.loss_cls: 0.1712  decode.d5.loss_mask: 0.7655  decode.d5.loss_dice: 0.5175  decode.d6.loss_cls: 0.1088  decode.d6.loss_mask: 0.9675  decode.d6.loss_dice: 0.5828  decode.d7.loss_cls: 0.0886  decode.d7.loss_mask: 0.9481  decode.d7.loss_dice: 0.6073  decode.d8.loss_cls: 0.1246  decode.d8.loss_mask: 0.8742  decode.d8.loss_dice: 0.5562
2023/05/23 07:20:26 - mmengine - INFO - Saving checkpoint at 1000 iterations
2023/05/23 07:21:13 - mmengine - INFO - Epoch(train) [1][1050/1297]  lr: 1.0000e-04  eta: 1:08:51  time: 0.8029  data_time: 0.0142  memory: 2835  grad_norm: 319.6116  loss: 24.6114  decode.loss_cls: 0.5239  decode.loss_mask: 1.1417  decode.loss_dice: 0.5580  decode.d0.loss_cls: 0.4405  decode.d0.loss_mask: 1.2628  decode.d0.loss_dice: 0.6271  decode.d1.loss_cls: 0.5171  decode.d1.loss_mask: 1.2157  decode.d1.loss_dice: 0.5745  decode.d2.loss_cls: 0.6147  decode.d2.loss_mask: 1.0978  decode.d2.loss_dice: 0.6116  decode.d3.loss_cls: 0.4892  decode.d3.loss_mask: 1.4179  decode.d3.loss_dice: 0.6968  decode.d4.loss_cls: 0.5235  decode.d4.loss_mask: 1.2878  decode.d4.loss_dice: 0.6858  decode.d5.loss_cls: 0.4978  decode.d5.loss_mask: 1.2222  decode.d5.loss_dice: 0.6550  decode.d6.loss_cls: 0.4583  decode.d6.loss_mask: 1.2576  decode.d6.loss_dice: 0.6832  decode.d7.loss_cls: 0.5227  decode.d7.loss_mask: 1.3095  decode.d7.loss_dice: 0.7030  decode.d8.loss_cls: 0.4700  decode.d8.loss_mask: 1.8140  decode.d8.loss_dice: 0.7321
2023/05/23 07:21:50 - mmengine - INFO - Epoch(train) [1][1100/1297]  lr: 1.0000e-04  eta: 1:08:11  time: 0.6384  data_time: 0.0611  memory: 2830  grad_norm: 217.9524  loss: 14.7640  decode.loss_cls: 0.1802  decode.loss_mask: 0.7418  decode.loss_dice: 0.5523  decode.d0.loss_cls: 0.2337  decode.d0.loss_mask: 0.7790  decode.d0.loss_dice: 0.5224  decode.d1.loss_cls: 0.1540  decode.d1.loss_mask: 0.6978  decode.d1.loss_dice: 0.4782  decode.d2.loss_cls: 0.1143  decode.d2.loss_mask: 0.8542  decode.d2.loss_dice: 0.5187  decode.d3.loss_cls: 0.1175  decode.d3.loss_mask: 0.9425  decode.d3.loss_dice: 0.6447  decode.d4.loss_cls: 0.1264  decode.d4.loss_mask: 1.0518  decode.d4.loss_dice: 0.6741  decode.d5.loss_cls: 0.2451  decode.d5.loss_mask: 0.7181  decode.d5.loss_dice: 0.4470  decode.d6.loss_cls: 0.2108  decode.d6.loss_mask: 0.6514  decode.d6.loss_dice: 0.4106  decode.d7.loss_cls: 0.1911  decode.d7.loss_mask: 0.6926  decode.d7.loss_dice: 0.4454  decode.d8.loss_cls: 0.2059  decode.d8.loss_mask: 0.7048  decode.d8.loss_dice: 0.4575
2023/05/23 07:22:26 - mmengine - INFO - Epoch(train) [1][1150/1297]  lr: 1.0000e-04  eta: 1:07:21  time: 0.7241  data_time: 0.0593  memory: 2829  grad_norm: 185.3271  loss: 20.6897  decode.loss_cls: 0.1753  decode.loss_mask: 0.9450  decode.loss_dice: 0.9986  decode.d0.loss_cls: 0.2244  decode.d0.loss_mask: 0.8745  decode.d0.loss_dice: 0.6927  decode.d1.loss_cls: 0.2417  decode.d1.loss_mask: 0.7179  decode.d1.loss_dice: 0.8300  decode.d2.loss_cls: 0.1792  decode.d2.loss_mask: 0.9063  decode.d2.loss_dice: 0.9734  decode.d3.loss_cls: 0.1800  decode.d3.loss_mask: 1.0200  decode.d3.loss_dice: 0.9524  decode.d4.loss_cls: 0.1651  decode.d4.loss_mask: 0.9783  decode.d4.loss_dice: 0.9809  decode.d5.loss_cls: 0.2329  decode.d5.loss_mask: 0.8951  decode.d5.loss_dice: 0.8253  decode.d6.loss_cls: 0.2063  decode.d6.loss_mask: 0.9536  decode.d6.loss_dice: 1.0042  decode.d7.loss_cls: 0.1939  decode.d7.loss_mask: 0.9924  decode.d7.loss_dice: 1.0283  decode.d8.loss_cls: 0.1866  decode.d8.loss_mask: 1.0878  decode.d8.loss_dice: 1.0476
2023/05/23 07:23:03 - mmengine - INFO - Epoch(train) [1][1200/1297]  lr: 1.0000e-04  eta: 1:06:40  time: 0.8699  data_time: 0.0136  memory: 2829  grad_norm: 419.8394  loss: 24.1943  decode.loss_cls: 0.1379  decode.loss_mask: 1.8285  decode.loss_dice: 0.5949  decode.d0.loss_cls: 0.1580  decode.d0.loss_mask: 1.5790  decode.d0.loss_dice: 0.7609  decode.d1.loss_cls: 0.3260  decode.d1.loss_mask: 1.2671  decode.d1.loss_dice: 0.6888  decode.d2.loss_cls: 0.2731  decode.d2.loss_mask: 1.4400  decode.d2.loss_dice: 0.7880  decode.d3.loss_cls: 0.1985  decode.d3.loss_mask: 1.4447  decode.d3.loss_dice: 0.6282  decode.d4.loss_cls: 0.1754  decode.d4.loss_mask: 1.4388  decode.d4.loss_dice: 0.6297  decode.d5.loss_cls: 0.2410  decode.d5.loss_mask: 1.5073  decode.d5.loss_dice: 0.6737  decode.d6.loss_cls: 0.1523  decode.d6.loss_mask: 1.7610  decode.d6.loss_dice: 0.5501  decode.d7.loss_cls: 0.1566  decode.d7.loss_mask: 1.7525  decode.d7.loss_dice: 0.5864  decode.d8.loss_cls: 0.1593  decode.d8.loss_mask: 1.7128  decode.d8.loss_dice: 0.5835
2023/05/23 07:23:37 - mmengine - INFO - Epoch(train) [1][1250/1297]  lr: 1.0000e-04  eta: 1:05:48  time: 0.8379  data_time: 0.0150  memory: 2830  grad_norm: 207.1158  loss: 21.5185  decode.loss_cls: 0.4701  decode.loss_mask: 1.0308  decode.loss_dice: 0.6410  decode.d0.loss_cls: 0.3690  decode.d0.loss_mask: 1.0618  decode.d0.loss_dice: 0.7488  decode.d1.loss_cls: 0.5322  decode.d1.loss_mask: 0.9784  decode.d1.loss_dice: 0.7105  decode.d2.loss_cls: 0.5662  decode.d2.loss_mask: 1.2029  decode.d2.loss_dice: 0.8713  decode.d3.loss_cls: 0.4811  decode.d3.loss_mask: 0.9448  decode.d3.loss_dice: 0.6273  decode.d4.loss_cls: 0.4383  decode.d4.loss_mask: 0.9684  decode.d4.loss_dice: 0.6313  decode.d5.loss_cls: 0.5095  decode.d5.loss_mask: 0.8998  decode.d5.loss_dice: 0.6228  decode.d6.loss_cls: 0.4698  decode.d6.loss_mask: 0.9630  decode.d6.loss_dice: 0.6360  decode.d7.loss_cls: 0.4478  decode.d7.loss_mask: 0.9623  decode.d7.loss_dice: 0.6622  decode.d8.loss_cls: 0.4489  decode.d8.loss_mask: 0.9769  decode.d8.loss_dice: 0.6456
2023/05/23 07:23:37 - mmengine - INFO - Saving checkpoint at 1250 iterations
2023/05/23 07:24:15 - mmengine - INFO - Exp name: mask2former_r50_8xb2-160k_ade20k-512x512_20230523_070719
2023/05/23 07:24:16 - mmengine - INFO - Epoch(train) [1][1300/1297]  lr: 1.0000e-04  eta: 1:05:15  time: 0.5576  data_time: 0.0086  memory: 2829  grad_norm: 223.7758  loss: 15.3014  decode.loss_cls: 0.1298  decode.loss_mask: 0.9349  decode.loss_dice: 0.3741  decode.d0.loss_cls: 0.1423  decode.d0.loss_mask: 0.9184  decode.d0.loss_dice: 0.3935  decode.d1.loss_cls: 0.1758  decode.d1.loss_mask: 1.0636  decode.d1.loss_dice: 0.3663  decode.d2.loss_cls: 0.1715  decode.d2.loss_mask: 1.0921  decode.d2.loss_dice: 0.3673  decode.d3.loss_cls: 0.1708  decode.d3.loss_mask: 1.1560  decode.d3.loss_dice: 0.4917  decode.d4.loss_cls: 0.1968  decode.d4.loss_mask: 0.8897  decode.d4.loss_dice: 0.4226  decode.d5.loss_cls: 0.1312  decode.d5.loss_mask: 1.0037  decode.d5.loss_dice: 0.4286  decode.d6.loss_cls: 0.2424  decode.d6.loss_mask: 0.7921  decode.d6.loss_dice: 0.3314  decode.d7.loss_cls: 0.1545  decode.d7.loss_mask: 0.9322  decode.d7.loss_dice: 0.3607  decode.d8.loss_cls: 0.1427  decode.d8.loss_mask: 0.9506  decode.d8.loss_dice: 0.3743
2023/05/23 07:24:53 - mmengine - INFO - Epoch(train) [1][1350/1297]  lr: 1.0000e-04  eta: 1:04:35  time: 0.8290  data_time: 0.0200  memory: 2829  grad_norm: 239.7781  loss: 16.7822  decode.loss_cls: 0.3391  decode.loss_mask: 1.0197  decode.loss_dice: 0.5078  decode.d0.loss_cls: 0.3460  decode.d0.loss_mask: 0.7225  decode.d0.loss_dice: 0.3075  decode.d1.loss_cls: 0.2731  decode.d1.loss_mask: 0.6933  decode.d1.loss_dice: 0.4433  decode.d2.loss_cls: 0.2953  decode.d2.loss_mask: 0.7915  decode.d2.loss_dice: 0.4807  decode.d3.loss_cls: 0.3219  decode.d3.loss_mask: 0.7404  decode.d3.loss_dice: 0.5026  decode.d4.loss_cls: 0.3455  decode.d4.loss_mask: 0.6974  decode.d4.loss_dice: 0.5541  decode.d5.loss_cls: 0.4135  decode.d5.loss_mask: 1.0787  decode.d5.loss_dice: 0.5686  decode.d6.loss_cls: 0.3750  decode.d6.loss_mask: 1.0066  decode.d6.loss_dice: 0.4943  decode.d7.loss_cls: 0.3651  decode.d7.loss_mask: 0.8425  decode.d7.loss_dice: 0.4700  decode.d8.loss_cls: 0.3983  decode.d8.loss_mask: 0.9905  decode.d8.loss_dice: 0.3978
2023/05/23 07:25:33 - mmengine - INFO - Epoch(train) [1][1400/1297]  lr: 1.0000e-04  eta: 1:04:05  time: 0.7473  data_time: 0.1044  memory: 2829  grad_norm: 333.5512  loss: 21.3448  decode.loss_cls: 0.2609  decode.loss_mask: 1.0100  decode.loss_dice: 0.8192  decode.d0.loss_cls: 0.4124  decode.d0.loss_mask: 0.9271  decode.d0.loss_dice: 0.8246  decode.d1.loss_cls: 0.3036  decode.d1.loss_mask: 0.9248  decode.d1.loss_dice: 0.7255  decode.d2.loss_cls: 0.3108  decode.d2.loss_mask: 0.9082  decode.d2.loss_dice: 0.7896  decode.d3.loss_cls: 0.4479  decode.d3.loss_mask: 0.9254  decode.d3.loss_dice: 0.8182  decode.d4.loss_cls: 0.3090  decode.d4.loss_mask: 1.0894  decode.d4.loss_dice: 0.8787  decode.d5.loss_cls: 0.2961  decode.d5.loss_mask: 1.1143  decode.d5.loss_dice: 0.8595  decode.d6.loss_cls: 0.3114  decode.d6.loss_mask: 0.9554  decode.d6.loss_dice: 0.7983  decode.d7.loss_cls: 0.3276  decode.d7.loss_mask: 0.9580  decode.d7.loss_dice: 0.8250  decode.d8.loss_cls: 0.2876  decode.d8.loss_mask: 1.0909  decode.d8.loss_dice: 0.8355
2023/05/23 07:26:07 - mmengine - INFO - Epoch(train) [1][1450/1297]  lr: 1.0000e-04  eta: 1:03:13  time: 0.5809  data_time: 0.0071  memory: 2840  grad_norm: 394.0191  loss: 24.0932  decode.loss_cls: 0.3147  decode.loss_mask: 2.1778  decode.loss_dice: 0.7071  decode.d0.loss_cls: 0.3437  decode.d0.loss_mask: 1.3398  decode.d0.loss_dice: 0.6761  decode.d1.loss_cls: 0.2431  decode.d1.loss_mask: 1.3051  decode.d1.loss_dice: 0.7275  decode.d2.loss_cls: 0.2449  decode.d2.loss_mask: 1.3218  decode.d2.loss_dice: 0.6621  decode.d3.loss_cls: 0.2750  decode.d3.loss_mask: 1.2187  decode.d3.loss_dice: 0.6996  decode.d4.loss_cls: 0.2848  decode.d4.loss_mask: 1.1998  decode.d4.loss_dice: 0.7141  decode.d5.loss_cls: 0.2908  decode.d5.loss_mask: 1.2230  decode.d5.loss_dice: 0.7728  decode.d6.loss_cls: 0.3023  decode.d6.loss_mask: 1.2869  decode.d6.loss_dice: 0.6884  decode.d7.loss_cls: 0.2954  decode.d7.loss_mask: 1.6257  decode.d7.loss_dice: 0.7907  decode.d8.loss_cls: 0.2699  decode.d8.loss_mask: 1.3530  decode.d8.loss_dice: 0.7386
2023/05/23 07:26:44 - mmengine - INFO - Epoch(train) [1][1500/1297]  lr: 1.0000e-04  eta: 1:02:32  time: 0.5471  data_time: 0.0070  memory: 2829  grad_norm: 391.5746  loss: 18.3033  decode.loss_cls: 0.2180  decode.loss_mask: 0.9385  decode.loss_dice: 0.3300  decode.d0.loss_cls: 0.2968  decode.d0.loss_mask: 1.0079  decode.d0.loss_dice: 0.3390  decode.d1.loss_cls: 0.1828  decode.d1.loss_mask: 1.2757  decode.d1.loss_dice: 0.3758  decode.d2.loss_cls: 0.2017  decode.d2.loss_mask: 1.3945  decode.d2.loss_dice: 0.4531  decode.d3.loss_cls: 0.1997  decode.d3.loss_mask: 1.5177  decode.d3.loss_dice: 0.3503  decode.d4.loss_cls: 0.2158  decode.d4.loss_mask: 1.0880  decode.d4.loss_dice: 0.4004  decode.d5.loss_cls: 0.1826  decode.d5.loss_mask: 1.5641  decode.d5.loss_dice: 0.3728  decode.d6.loss_cls: 0.2435  decode.d6.loss_mask: 1.6592  decode.d6.loss_dice: 0.3608  decode.d7.loss_cls: 0.1619  decode.d7.loss_mask: 0.9911  decode.d7.loss_dice: 0.3627  decode.d8.loss_cls: 0.2199  decode.d8.loss_mask: 1.0483  decode.d8.loss_dice: 0.3508
2023/05/23 07:26:44 - mmengine - INFO - Saving checkpoint at 1500 iterations
2023/05/23 07:27:29 - mmengine - INFO - Epoch(train) [1][1550/1297]  lr: 1.0000e-04  eta: 1:02:20  time: 0.5811  data_time: 0.0163  memory: 2829  grad_norm: 613.5032  loss: 23.7809  decode.loss_cls: 0.3903  decode.loss_mask: 1.2610  decode.loss_dice: 0.7674  decode.d0.loss_cls: 0.3070  decode.d0.loss_mask: 0.9470  decode.d0.loss_dice: 0.7439  decode.d1.loss_cls: 0.5662  decode.d1.loss_mask: 1.0734  decode.d1.loss_dice: 0.7633  decode.d2.loss_cls: 0.5211  decode.d2.loss_mask: 2.3632  decode.d2.loss_dice: 0.7395  decode.d3.loss_cls: 0.5002  decode.d3.loss_mask: 1.0439  decode.d3.loss_dice: 0.7625  decode.d4.loss_cls: 0.4877  decode.d4.loss_mask: 0.9925  decode.d4.loss_dice: 0.7311  decode.d5.loss_cls: 0.4066  decode.d5.loss_mask: 1.0586  decode.d5.loss_dice: 0.7287  decode.d6.loss_cls: 0.4335  decode.d6.loss_mask: 1.0607  decode.d6.loss_dice: 0.7774  decode.d7.loss_cls: 0.4159  decode.d7.loss_mask: 1.0270  decode.d7.loss_dice: 0.7406  decode.d8.loss_cls: 0.3925  decode.d8.loss_mask: 1.0399  decode.d8.loss_dice: 0.7383
2023/05/23 07:28:02 - mmengine - INFO - Epoch(train) [1][1600/1297]  lr: 1.0000e-04  eta: 1:01:28  time: 0.5548  data_time: 0.0064  memory: 2830  grad_norm: 187.1750  loss: 18.9417  decode.loss_cls: 0.4012  decode.loss_mask: 0.8795  decode.loss_dice: 0.5336  decode.d0.loss_cls: 0.5247  decode.d0.loss_mask: 0.8458  decode.d0.loss_dice: 0.5484  decode.d1.loss_cls: 0.4348  decode.d1.loss_mask: 0.9557  decode.d1.loss_dice: 0.5087  decode.d2.loss_cls: 0.4051  decode.d2.loss_mask: 0.8836  decode.d2.loss_dice: 0.5073  decode.d3.loss_cls: 0.4769  decode.d3.loss_mask: 0.8709  decode.d3.loss_dice: 0.5634  decode.d4.loss_cls: 0.4747  decode.d4.loss_mask: 0.8699  decode.d4.loss_dice: 0.5244  decode.d5.loss_cls: 0.4667  decode.d5.loss_mask: 0.9602  decode.d5.loss_dice: 0.5374  decode.d6.loss_cls: 0.4660  decode.d6.loss_mask: 0.9700  decode.d6.loss_dice: 0.5263  decode.d7.loss_cls: 0.4822  decode.d7.loss_mask: 0.8581  decode.d7.loss_dice: 0.5809  decode.d8.loss_cls: 0.4798  decode.d8.loss_mask: 0.8431  decode.d8.loss_dice: 0.5626
2023/05/23 07:28:38 - mmengine - INFO - Epoch(train) [1][1650/1297]  lr: 1.0000e-04  eta: 1:00:44  time: 0.7674  data_time: 0.0753  memory: 2829  grad_norm: 147.4427  loss: 12.5025  decode.loss_cls: 0.2340  decode.loss_mask: 0.6622  decode.loss_dice: 0.3747  decode.d0.loss_cls: 0.1873  decode.d0.loss_mask: 0.6008  decode.d0.loss_dice: 0.3442  decode.d1.loss_cls: 0.2391  decode.d1.loss_mask: 0.6134  decode.d1.loss_dice: 0.3481  decode.d2.loss_cls: 0.2765  decode.d2.loss_mask: 0.6063  decode.d2.loss_dice: 0.3454  decode.d3.loss_cls: 0.1346  decode.d3.loss_mask: 0.9864  decode.d3.loss_dice: 0.4097  decode.d4.loss_cls: 0.2325  decode.d4.loss_mask: 0.6876  decode.d4.loss_dice: 0.3980  decode.d5.loss_cls: 0.2272  decode.d5.loss_mask: 0.6531  decode.d5.loss_dice: 0.3555  decode.d6.loss_cls: 0.2234  decode.d6.loss_mask: 0.6717  decode.d6.loss_dice: 0.3901  decode.d7.loss_cls: 0.2677  decode.d7.loss_mask: 0.5378  decode.d7.loss_dice: 0.3112  decode.d8.loss_cls: 0.2126  decode.d8.loss_mask: 0.6045  decode.d8.loss_dice: 0.3670
2023/05/23 07:29:15 - mmengine - INFO - Epoch(train) [1][1700/1297]  lr: 1.0000e-04  eta: 1:00:02  time: 0.7565  data_time: 0.0136  memory: 2829  grad_norm: 179.9979  loss: 22.3419  decode.loss_cls: 0.5173  decode.loss_mask: 0.8788  decode.loss_dice: 0.8206  decode.d0.loss_cls: 0.5200  decode.d0.loss_mask: 0.9085  decode.d0.loss_dice: 0.8357  decode.d1.loss_cls: 0.4853  decode.d1.loss_mask: 0.8670  decode.d1.loss_dice: 0.8301  decode.d2.loss_cls: 0.4620  decode.d2.loss_mask: 0.8495  decode.d2.loss_dice: 0.7716  decode.d3.loss_cls: 0.5404  decode.d3.loss_mask: 0.9313  decode.d3.loss_dice: 0.8979  decode.d4.loss_cls: 0.3973  decode.d4.loss_mask: 0.9593  decode.d4.loss_dice: 0.9879  decode.d5.loss_cls: 0.4607  decode.d5.loss_mask: 0.8494  decode.d5.loss_dice: 0.8670  decode.d6.loss_cls: 0.4721  decode.d6.loss_mask: 0.8418  decode.d6.loss_dice: 0.8187  decode.d7.loss_cls: 0.5035  decode.d7.loss_mask: 0.8554  decode.d7.loss_dice: 0.8598  decode.d8.loss_cls: 0.4956  decode.d8.loss_mask: 0.9659  decode.d8.loss_dice: 0.8914
2023/05/23 07:29:50 - mmengine - INFO - Epoch(train) [1][1750/1297]  lr: 1.0000e-04  eta: 0:59:18  time: 0.6606  data_time: 0.0098  memory: 2829  grad_norm: 92.4209  loss: 14.6977  decode.loss_cls: 0.3687  decode.loss_mask: 0.4208  decode.loss_dice: 0.5600  decode.d0.loss_cls: 0.4754  decode.d0.loss_mask: 0.5399  decode.d0.loss_dice: 0.6195  decode.d1.loss_cls: 0.3470  decode.d1.loss_mask: 0.5560  decode.d1.loss_dice: 0.6593  decode.d2.loss_cls: 0.3717  decode.d2.loss_mask: 0.5698  decode.d2.loss_dice: 0.6359  decode.d3.loss_cls: 0.3969  decode.d3.loss_mask: 0.4170  decode.d3.loss_dice: 0.5411  decode.d4.loss_cls: 0.2975  decode.d4.loss_mask: 0.6425  decode.d4.loss_dice: 0.6800  decode.d5.loss_cls: 0.3333  decode.d5.loss_mask: 0.5550  decode.d5.loss_dice: 0.6697  decode.d6.loss_cls: 0.3726  decode.d6.loss_mask: 0.4367  decode.d6.loss_dice: 0.5553  decode.d7.loss_cls: 0.3795  decode.d7.loss_mask: 0.4103  decode.d7.loss_dice: 0.5611  decode.d8.loss_cls: 0.3294  decode.d8.loss_mask: 0.4301  decode.d8.loss_dice: 0.5658
2023/05/23 07:29:50 - mmengine - INFO - Saving checkpoint at 1750 iterations
2023/05/23 07:30:27 - mmengine - INFO - Epoch(train) [1][1800/1297]  lr: 1.0000e-04  eta: 0:58:40  time: 0.6027  data_time: 0.0087  memory: 2829  grad_norm: 337.6172  loss: 23.5020  decode.loss_cls: 0.3206  decode.loss_mask: 1.4000  decode.loss_dice: 0.7211  decode.d0.loss_cls: 0.4667  decode.d0.loss_mask: 1.0965  decode.d0.loss_dice: 0.5659  decode.d1.loss_cls: 0.3484  decode.d1.loss_mask: 1.1141  decode.d1.loss_dice: 0.5321  decode.d2.loss_cls: 0.2718  decode.d2.loss_mask: 1.1794  decode.d2.loss_dice: 0.6727  decode.d3.loss_cls: 0.2725  decode.d3.loss_mask: 1.2646  decode.d3.loss_dice: 0.5445  decode.d4.loss_cls: 0.2552  decode.d4.loss_mask: 1.1480  decode.d4.loss_dice: 0.5907  decode.d5.loss_cls: 0.2674  decode.d5.loss_mask: 1.4250  decode.d5.loss_dice: 0.8208  decode.d6.loss_cls: 0.2730  decode.d6.loss_mask: 2.3698  decode.d6.loss_dice: 0.7345  decode.d7.loss_cls: 0.2509  decode.d7.loss_mask: 1.3694  decode.d7.loss_dice: 0.7182  decode.d8.loss_cls: 0.2815  decode.d8.loss_mask: 1.5444  decode.d8.loss_dice: 0.6821
2023/05/23 07:31:08 - mmengine - INFO - Epoch(train) [1][1850/1297]  lr: 1.0000e-04  eta: 0:58:11  time: 0.8035  data_time: 0.0691  memory: 2829  grad_norm: 244.0821  loss: 18.2744  decode.loss_cls: 0.3265  decode.loss_mask: 0.8637  decode.loss_dice: 0.5176  decode.d0.loss_cls: 0.3689  decode.d0.loss_mask: 0.7916  decode.d0.loss_dice: 0.4393  decode.d1.loss_cls: 0.3545  decode.d1.loss_mask: 1.7836  decode.d1.loss_dice: 1.0041  decode.d2.loss_cls: 0.3450  decode.d2.loss_mask: 0.9173  decode.d2.loss_dice: 0.6126  decode.d3.loss_cls: 0.3362  decode.d3.loss_mask: 0.8585  decode.d3.loss_dice: 0.4839  decode.d4.loss_cls: 0.3066  decode.d4.loss_mask: 0.8417  decode.d4.loss_dice: 0.5362  decode.d5.loss_cls: 0.3065  decode.d5.loss_mask: 0.8634  decode.d5.loss_dice: 0.5222  decode.d6.loss_cls: 0.3249  decode.d6.loss_mask: 0.8393  decode.d6.loss_dice: 0.5164  decode.d7.loss_cls: 0.2973  decode.d7.loss_mask: 0.8211  decode.d7.loss_dice: 0.4806  decode.d8.loss_cls: 0.3366  decode.d8.loss_mask: 0.8240  decode.d8.loss_dice: 0.4546
2023/05/23 07:31:40 - mmengine - INFO - Epoch(train) [1][1900/1297]  lr: 1.0000e-04  eta: 0:57:19  time: 0.5888  data_time: 0.0076  memory: 2829  grad_norm: 281.7686  loss: 24.6551  decode.loss_cls: 0.3784  decode.loss_mask: 1.2347  decode.loss_dice: 0.6108  decode.d0.loss_cls: 0.3735  decode.d0.loss_mask: 1.2792  decode.d0.loss_dice: 0.6252  decode.d1.loss_cls: 0.2492  decode.d1.loss_mask: 1.7134  decode.d1.loss_dice: 0.7633  decode.d2.loss_cls: 0.2273  decode.d2.loss_mask: 1.5644  decode.d2.loss_dice: 0.7034  decode.d3.loss_cls: 0.2498  decode.d3.loss_mask: 1.5988  decode.d3.loss_dice: 0.7041  decode.d4.loss_cls: 0.2214  decode.d4.loss_mask: 1.6091  decode.d4.loss_dice: 0.7109  decode.d5.loss_cls: 0.3064  decode.d5.loss_mask: 1.5975  decode.d5.loss_dice: 0.6856  decode.d6.loss_cls: 0.2929  decode.d6.loss_mask: 1.6225  decode.d6.loss_dice: 0.6787  decode.d7.loss_cls: 0.3918  decode.d7.loss_mask: 1.3770  decode.d7.loss_dice: 0.6524  decode.d8.loss_cls: 0.3575  decode.d8.loss_mask: 1.2650  decode.d8.loss_dice: 0.6109
2023/05/23 07:32:16 - mmengine - INFO - Epoch(train) [1][1950/1297]  lr: 1.0000e-04  eta: 0:56:38  time: 0.6404  data_time: 0.0307  memory: 2830  grad_norm: 308.3763  loss: 31.1064  decode.loss_cls: 0.3594  decode.loss_mask: 1.8843  decode.loss_dice: 0.9132  decode.d0.loss_cls: 0.5269  decode.d0.loss_mask: 1.9812  decode.d0.loss_dice: 0.7500  decode.d1.loss_cls: 0.2536  decode.d1.loss_mask: 2.2967  decode.d1.loss_dice: 0.7510  decode.d2.loss_cls: 0.3245  decode.d2.loss_mask: 2.0055  decode.d2.loss_dice: 0.7378  decode.d3.loss_cls: 0.2382  decode.d3.loss_mask: 2.4913  decode.d3.loss_dice: 0.7300  decode.d4.loss_cls: 0.3610  decode.d4.loss_mask: 1.7539  decode.d4.loss_dice: 0.6893  decode.d5.loss_cls: 0.3479  decode.d5.loss_mask: 1.8051  decode.d5.loss_dice: 0.7193  decode.d6.loss_cls: 0.3998  decode.d6.loss_mask: 1.7401  decode.d6.loss_dice: 0.7055  decode.d7.loss_cls: 0.3087  decode.d7.loss_mask: 2.1567  decode.d7.loss_dice: 0.8240  decode.d8.loss_cls: 0.3246  decode.d8.loss_mask: 1.9788  decode.d8.loss_dice: 0.7482
2023/05/23 07:32:53 - mmengine - INFO - Exp name: mask2former_r50_8xb2-160k_ade20k-512x512_20230523_070719
2023/05/23 07:32:53 - mmengine - INFO - Epoch(train) [1][2000/1297]  lr: 1.0000e-04  eta: 0:56:00  time: 0.7926  data_time: 0.0127  memory: 2830  grad_norm: 137.9115  loss: 14.5248  decode.loss_cls: 0.0599  decode.loss_mask: 0.9990  decode.loss_dice: 0.3609  decode.d0.loss_cls: 0.1058  decode.d0.loss_mask: 0.9967  decode.d0.loss_dice: 0.3655  decode.d1.loss_cls: 0.0781  decode.d1.loss_mask: 0.9984  decode.d1.loss_dice: 0.3761  decode.d2.loss_cls: 0.0642  decode.d2.loss_mask: 0.9613  decode.d2.loss_dice: 0.3788  decode.d3.loss_cls: 0.0751  decode.d3.loss_mask: 0.9756  decode.d3.loss_dice: 0.3734  decode.d4.loss_cls: 0.0683  decode.d4.loss_mask: 1.0000  decode.d4.loss_dice: 0.3741  decode.d5.loss_cls: 0.0771  decode.d5.loss_mask: 1.0667  decode.d5.loss_dice: 0.3764  decode.d6.loss_cls: 0.0799  decode.d6.loss_mask: 1.0074  decode.d6.loss_dice: 0.3717  decode.d7.loss_cls: 0.0674  decode.d7.loss_mask: 1.0413  decode.d7.loss_dice: 0.3867  decode.d8.loss_cls: 0.0752  decode.d8.loss_mask: 0.9956  decode.d8.loss_dice: 0.3683
2023/05/23 07:32:53 - mmengine - INFO - Saving checkpoint at 2000 iterations
2023/05/23 07:33:32 - mmengine - INFO - Epoch(train) [1][2050/1297]  lr: 1.0000e-04  eta: 0:55:26  time: 0.6251  data_time: 0.0093  memory: 2830  grad_norm: 266.3401  loss: 18.8478  decode.loss_cls: 0.4422  decode.loss_mask: 0.7357  decode.loss_dice: 0.6897  decode.d0.loss_cls: 0.3407  decode.d0.loss_mask: 0.6901  decode.d0.loss_dice: 0.6083  decode.d1.loss_cls: 0.4962  decode.d1.loss_mask: 0.7884  decode.d1.loss_dice: 0.7166  decode.d2.loss_cls: 0.4129  decode.d2.loss_mask: 0.9776  decode.d2.loss_dice: 0.8334  decode.d3.loss_cls: 0.5248  decode.d3.loss_mask: 0.7371  decode.d3.loss_dice: 0.6746  decode.d4.loss_cls: 0.4615  decode.d4.loss_mask: 0.7438  decode.d4.loss_dice: 0.6700  decode.d5.loss_cls: 0.4402  decode.d5.loss_mask: 0.7098  decode.d5.loss_dice: 0.6957  decode.d6.loss_cls: 0.3873  decode.d6.loss_mask: 0.7410  decode.d6.loss_dice: 0.6713  decode.d7.loss_cls: 0.3782  decode.d7.loss_mask: 0.7573  decode.d7.loss_dice: 0.6684  decode.d8.loss_cls: 0.3902  decode.d8.loss_mask: 0.7882  decode.d8.loss_dice: 0.6767
2023/05/23 07:34:07 - mmengine - INFO - Epoch(train) [1][2100/1297]  lr: 1.0000e-04  eta: 0:54:42  time: 0.5542  data_time: 0.0075  memory: 2848  grad_norm: 725.6054  loss: 29.3196  decode.loss_cls: 0.3315  decode.loss_mask: 1.0679  decode.loss_dice: 0.5781  decode.d0.loss_cls: 0.3899  decode.d0.loss_mask: 0.9641  decode.d0.loss_dice: 0.5446  decode.d1.loss_cls: 0.4385  decode.d1.loss_mask: 0.8459  decode.d1.loss_dice: 0.4632  decode.d2.loss_cls: 0.3298  decode.d2.loss_mask: 1.3156  decode.d2.loss_dice: 0.8543  decode.d3.loss_cls: 0.3423  decode.d3.loss_mask: 2.2434  decode.d3.loss_dice: 0.9798  decode.d4.loss_cls: 0.3787  decode.d4.loss_mask: 1.8107  decode.d4.loss_dice: 1.0603  decode.d5.loss_cls: 0.3614  decode.d5.loss_mask: 3.1363  decode.d5.loss_dice: 1.2840  decode.d6.loss_cls: 0.2775  decode.d6.loss_mask: 2.9744  decode.d6.loss_dice: 1.1520  decode.d7.loss_cls: 0.3666  decode.d7.loss_mask: 2.0611  decode.d7.loss_dice: 0.7766  decode.d8.loss_cls: 0.3126  decode.d8.loss_mask: 1.0861  decode.d8.loss_dice: 0.5928
2023/05/23 07:34:42 - mmengine - INFO - Epoch(train) [1][2150/1297]  lr: 1.0000e-04  eta: 0:54:01  time: 0.6852  data_time: 0.0135  memory: 2829  grad_norm: 65.2204  loss: 8.9166  decode.loss_cls: 0.2051  decode.loss_mask: 0.4339  decode.loss_dice: 0.2553  decode.d0.loss_cls: 0.2236  decode.d0.loss_mask: 0.4097  decode.d0.loss_dice: 0.2424  decode.d1.loss_cls: 0.1865  decode.d1.loss_mask: 0.4403  decode.d1.loss_dice: 0.2612  decode.d2.loss_cls: 0.1718  decode.d2.loss_mask: 0.4244  decode.d2.loss_dice: 0.2695  decode.d3.loss_cls: 0.2125  decode.d3.loss_mask: 0.4471  decode.d3.loss_dice: 0.2969  decode.d4.loss_cls: 0.1590  decode.d4.loss_mask: 0.4445  decode.d4.loss_dice: 0.2665  decode.d5.loss_cls: 0.1695  decode.d5.loss_mask: 0.4273  decode.d5.loss_dice: 0.2827  decode.d6.loss_cls: 0.2095  decode.d6.loss_mask: 0.4266  decode.d6.loss_dice: 0.2665  decode.d7.loss_cls: 0.2075  decode.d7.loss_mask: 0.4373  decode.d7.loss_dice: 0.2645  decode.d8.loss_cls: 0.1912  decode.d8.loss_mask: 0.4262  decode.d8.loss_dice: 0.2578
2023/05/23 07:35:18 - mmengine - INFO - Epoch(train) [1][2200/1297]  lr: 1.0000e-04  eta: 0:53:20  time: 0.8640  data_time: 0.0146  memory: 2829  grad_norm: 172.4707  loss: 26.1505  decode.loss_cls: 0.1969  decode.loss_mask: 1.5863  decode.loss_dice: 0.9355  decode.d0.loss_cls: 0.3033  decode.d0.loss_mask: 1.3328  decode.d0.loss_dice: 0.9442  decode.d1.loss_cls: 0.2454  decode.d1.loss_mask: 1.1982  decode.d1.loss_dice: 0.8379  decode.d2.loss_cls: 0.2237  decode.d2.loss_mask: 1.5439  decode.d2.loss_dice: 0.8628  decode.d3.loss_cls: 0.2131  decode.d3.loss_mask: 1.5795  decode.d3.loss_dice: 0.8380  decode.d4.loss_cls: 0.1685  decode.d4.loss_mask: 1.5794  decode.d4.loss_dice: 0.9313  decode.d5.loss_cls: 0.2291  decode.d5.loss_mask: 1.3074  decode.d5.loss_dice: 0.9566  decode.d6.loss_cls: 0.1933  decode.d6.loss_mask: 1.5844  decode.d6.loss_dice: 0.9330  decode.d7.loss_cls: 0.2423  decode.d7.loss_mask: 1.5825  decode.d7.loss_dice: 0.9321  decode.d8.loss_cls: 0.2648  decode.d8.loss_mask: 1.5562  decode.d8.loss_dice: 0.8481
2023/05/23 07:35:54 - mmengine - INFO - Epoch(train) [1][2250/1297]  lr: 1.0000e-04  eta: 0:52:41  time: 0.7887  data_time: 0.0118  memory: 2830  grad_norm: 183.7425  loss: 19.6516  decode.loss_cls: 0.2981  decode.loss_mask: 1.0481  decode.loss_dice: 0.5437  decode.d0.loss_cls: 0.3232  decode.d0.loss_mask: 1.1472  decode.d0.loss_dice: 0.5967  decode.d1.loss_cls: 0.3713  decode.d1.loss_mask: 1.2066  decode.d1.loss_dice: 0.6217  decode.d2.loss_cls: 0.2985  decode.d2.loss_mask: 1.0739  decode.d2.loss_dice: 0.5770  decode.d3.loss_cls: 0.2837  decode.d3.loss_mask: 1.0640  decode.d3.loss_dice: 0.5557  decode.d4.loss_cls: 0.2131  decode.d4.loss_mask: 1.0745  decode.d4.loss_dice: 0.6035  decode.d5.loss_cls: 0.2554  decode.d5.loss_mask: 1.1346  decode.d5.loss_dice: 0.6100  decode.d6.loss_cls: 0.3007  decode.d6.loss_mask: 1.0832  decode.d6.loss_dice: 0.5444  decode.d7.loss_cls: 0.2970  decode.d7.loss_mask: 1.0635  decode.d7.loss_dice: 0.5487  decode.d8.loss_cls: 0.2874  decode.d8.loss_mask: 1.0711  decode.d8.loss_dice: 0.5553
2023/05/23 07:35:54 - mmengine - INFO - Saving checkpoint at 2250 iterations
2023/05/23 07:36:36 - mmengine - INFO - Epoch(train) [1][2300/1297]  lr: 1.0000e-04  eta: 0:52:11  time: 0.6073  data_time: 0.0066  memory: 2830  grad_norm: 198.5636  loss: 18.7478  decode.loss_cls: 0.1924  decode.loss_mask: 0.9662  decode.loss_dice: 0.6905  decode.d0.loss_cls: 0.2425  decode.d0.loss_mask: 0.9323  decode.d0.loss_dice: 0.7342  decode.d1.loss_cls: 0.2430  decode.d1.loss_mask: 0.9422  decode.d1.loss_dice: 0.7123  decode.d2.loss_cls: 0.2151  decode.d2.loss_mask: 0.9111  decode.d2.loss_dice: 0.7064  decode.d3.loss_cls: 0.2174  decode.d3.loss_mask: 0.9524  decode.d3.loss_dice: 0.7066  decode.d4.loss_cls: 0.2260  decode.d4.loss_mask: 0.9554  decode.d4.loss_dice: 0.6984  decode.d5.loss_cls: 0.2257  decode.d5.loss_mask: 0.9595  decode.d5.loss_dice: 0.6866  decode.d6.loss_cls: 0.2188  decode.d6.loss_mask: 1.0154  decode.d6.loss_dice: 0.6772  decode.d7.loss_cls: 0.2100  decode.d7.loss_mask: 0.9535  decode.d7.loss_dice: 0.6913  decode.d8.loss_cls: 0.2146  decode.d8.loss_mask: 0.9701  decode.d8.loss_dice: 0.6806
2023/05/23 07:37:12 - mmengine - INFO - Epoch(train) [1][2350/1297]  lr: 1.0000e-04  eta: 0:51:32  time: 0.7976  data_time: 0.0135  memory: 2830  grad_norm: 253.4163  loss: 26.8838  decode.loss_cls: 0.4307  decode.loss_mask: 1.2417  decode.loss_dice: 0.9911  decode.d0.loss_cls: 0.5176  decode.d0.loss_mask: 1.0637  decode.d0.loss_dice: 0.9130  decode.d1.loss_cls: 0.4795  decode.d1.loss_mask: 1.0353  decode.d1.loss_dice: 0.8455  decode.d2.loss_cls: 0.4202  decode.d2.loss_mask: 1.2623  decode.d2.loss_dice: 0.9852  decode.d3.loss_cls: 0.5275  decode.d3.loss_mask: 1.3072  decode.d3.loss_dice: 0.8766  decode.d4.loss_cls: 0.4648  decode.d4.loss_mask: 1.3813  decode.d4.loss_dice: 1.0507  decode.d5.loss_cls: 0.4757  decode.d5.loss_mask: 1.4767  decode.d5.loss_dice: 1.0380  decode.d6.loss_cls: 0.4063  decode.d6.loss_mask: 1.4094  decode.d6.loss_dice: 1.0588  decode.d7.loss_cls: 0.4015  decode.d7.loss_mask: 1.2245  decode.d7.loss_dice: 0.9587  decode.d8.loss_cls: 0.4393  decode.d8.loss_mask: 1.2500  decode.d8.loss_dice: 0.9509
2023/05/23 07:37:46 - mmengine - INFO - Epoch(train) [1][2400/1297]  lr: 1.0000e-04  eta: 0:50:48  time: 0.7178  data_time: 0.0094  memory: 2829  grad_norm: 159.3265  loss: 15.4481  decode.loss_cls: 0.1283  decode.loss_mask: 0.8654  decode.loss_dice: 0.5309  decode.d0.loss_cls: 0.3242  decode.d0.loss_mask: 0.6609  decode.d0.loss_dice: 0.3438  decode.d1.loss_cls: 0.1958  decode.d1.loss_mask: 0.7700  decode.d1.loss_dice: 0.5132  decode.d2.loss_cls: 0.1620  decode.d2.loss_mask: 0.9211  decode.d2.loss_dice: 0.6467  decode.d3.loss_cls: 0.2239  decode.d3.loss_mask: 0.7942  decode.d3.loss_dice: 0.5327  decode.d4.loss_cls: 0.1932  decode.d4.loss_mask: 0.7484  decode.d4.loss_dice: 0.5092  decode.d5.loss_cls: 0.1669  decode.d5.loss_mask: 0.7887  decode.d5.loss_dice: 0.5332  decode.d6.loss_cls: 0.2189  decode.d6.loss_mask: 0.8488  decode.d6.loss_dice: 0.5543  decode.d7.loss_cls: 0.1273  decode.d7.loss_mask: 0.8982  decode.d7.loss_dice: 0.6257  decode.d8.loss_cls: 0.0816  decode.d8.loss_mask: 0.9142  decode.d8.loss_dice: 0.6265
2023/05/23 07:38:21 - mmengine - INFO - Epoch(train) [1][2450/1297]  lr: 1.0000e-04  eta: 0:50:08  time: 0.8260  data_time: 0.2028  memory: 2829  grad_norm: 94.4097  loss: 10.7283  decode.loss_cls: 0.0789  decode.loss_mask: 0.6507  decode.loss_dice: 0.3636  decode.d0.loss_cls: 0.1290  decode.d0.loss_mask: 0.5299  decode.d0.loss_dice: 0.3375  decode.d1.loss_cls: 0.0845  decode.d1.loss_mask: 0.6282  decode.d1.loss_dice: 0.3336  decode.d2.loss_cls: 0.1016  decode.d2.loss_mask: 0.6286  decode.d2.loss_dice: 0.3266  decode.d3.loss_cls: 0.0867  decode.d3.loss_mask: 0.6504  decode.d3.loss_dice: 0.3598  decode.d4.loss_cls: 0.1418  decode.d4.loss_mask: 0.6393  decode.d4.loss_dice: 0.3519  decode.d5.loss_cls: 0.1359  decode.d5.loss_mask: 0.6445  decode.d5.loss_dice: 0.3514  decode.d6.loss_cls: 0.1500  decode.d6.loss_mask: 0.5273  decode.d6.loss_dice: 0.3392  decode.d7.loss_cls: 0.0845  decode.d7.loss_mask: 0.6411  decode.d7.loss_dice: 0.3554  decode.d8.loss_cls: 0.0841  decode.d8.loss_mask: 0.6345  decode.d8.loss_dice: 0.3579
2023/05/23 07:38:56 - mmengine - INFO - Epoch(train) [1][2500/1297]  lr: 1.0000e-04  eta: 0:49:27  time: 0.6052  data_time: 0.0070  memory: 2829  grad_norm: 226.1303  loss: 19.6802  decode.loss_cls: 0.2464  decode.loss_mask: 1.0818  decode.loss_dice: 0.6391  decode.d0.loss_cls: 0.1669  decode.d0.loss_mask: 1.2419  decode.d0.loss_dice: 0.6653  decode.d1.loss_cls: 0.1784  decode.d1.loss_mask: 1.0657  decode.d1.loss_dice: 0.6140  decode.d2.loss_cls: 0.1558  decode.d2.loss_mask: 1.1620  decode.d2.loss_dice: 0.6212  decode.d3.loss_cls: 0.1835  decode.d3.loss_mask: 1.2019  decode.d3.loss_dice: 0.6192  decode.d4.loss_cls: 0.2062  decode.d4.loss_mask: 1.1701  decode.d4.loss_dice: 0.5969  decode.d5.loss_cls: 0.1935  decode.d5.loss_mask: 1.1746  decode.d5.loss_dice: 0.6298  decode.d6.loss_cls: 0.1739  decode.d6.loss_mask: 1.1368  decode.d6.loss_dice: 0.5888  decode.d7.loss_cls: 0.2531  decode.d7.loss_mask: 1.1053  decode.d7.loss_dice: 0.6485  decode.d8.loss_cls: 0.2583  decode.d8.loss_mask: 1.0921  decode.d8.loss_dice: 0.6091
2023/05/23 07:38:56 - mmengine - INFO - Saving checkpoint at 2500 iterations
2023/05/23 07:39:38 - mmengine - INFO - Epoch(train) [1][2550/1297]  lr: 1.0000e-04  eta: 0:48:56  time: 0.8896  data_time: 0.0992  memory: 2829  grad_norm: 289.9225  loss: 23.9979  decode.loss_cls: 0.5991  decode.loss_mask: 1.2029  decode.loss_dice: 0.4696  decode.d0.loss_cls: 0.4645  decode.d0.loss_mask: 1.2321  decode.d0.loss_dice: 0.4893  decode.d1.loss_cls: 0.5915  decode.d1.loss_mask: 1.3052  decode.d1.loss_dice: 0.5352  decode.d2.loss_cls: 0.4731  decode.d2.loss_mask: 1.3799  decode.d2.loss_dice: 0.5788  decode.d3.loss_cls: 0.4782  decode.d3.loss_mask: 1.3370  decode.d3.loss_dice: 0.5351  decode.d4.loss_cls: 0.4539  decode.d4.loss_mask: 1.3189  decode.d4.loss_dice: 0.5116  decode.d5.loss_cls: 0.4818  decode.d5.loss_mask: 1.3314  decode.d5.loss_dice: 0.4910  decode.d6.loss_cls: 0.4616  decode.d6.loss_mask: 1.4591  decode.d6.loss_dice: 0.5455  decode.d7.loss_cls: 0.4478  decode.d7.loss_mask: 1.8920  decode.d7.loss_dice: 0.6053  decode.d8.loss_cls: 0.4722  decode.d8.loss_mask: 1.3672  decode.d8.loss_dice: 0.4872
2023/05/23 07:40:13 - mmengine - INFO - Epoch(train) [1][2600/1297]  lr: 1.0000e-04  eta: 0:48:16  time: 0.7679  data_time: 0.0913  memory: 2829  grad_norm: 131.3006  loss: 18.8684  decode.loss_cls: 0.2107  decode.loss_mask: 1.1872  decode.loss_dice: 0.4558  decode.d0.loss_cls: 0.2207  decode.d0.loss_mask: 1.1362  decode.d0.loss_dice: 0.4534  decode.d1.loss_cls: 0.1609  decode.d1.loss_mask: 1.2060  decode.d1.loss_dice: 0.4471  decode.d2.loss_cls: 0.1604  decode.d2.loss_mask: 1.2431  decode.d2.loss_dice: 0.4682  decode.d3.loss_cls: 0.1507  decode.d3.loss_mask: 1.5979  decode.d3.loss_dice: 0.4513  decode.d4.loss_cls: 0.0803  decode.d4.loss_mask: 1.6725  decode.d4.loss_dice: 0.5237  decode.d5.loss_cls: 0.1561  decode.d5.loss_mask: 1.1108  decode.d5.loss_dice: 0.4434  decode.d6.loss_cls: 0.1586  decode.d6.loss_mask: 1.1310  decode.d6.loss_dice: 0.4575  decode.d7.loss_cls: 0.1579  decode.d7.loss_mask: 1.1584  decode.d7.loss_dice: 0.4486  decode.d8.loss_cls: 0.2866  decode.d8.loss_mask: 1.0826  decode.d8.loss_dice: 0.4508
2023/05/23 07:40:52 - mmengine - INFO - Epoch(train) [1][2650/1297]  lr: 1.0000e-04  eta: 0:47:41  time: 0.8524  data_time: 0.1276  memory: 2829  grad_norm: 313.2426  loss: 17.1625  decode.loss_cls: 0.2988  decode.loss_mask: 0.9766  decode.loss_dice: 0.7043  decode.d0.loss_cls: 0.3720  decode.d0.loss_mask: 0.6199  decode.d0.loss_dice: 0.4716  decode.d1.loss_cls: 0.2808  decode.d1.loss_mask: 0.6070  decode.d1.loss_dice: 0.4840  decode.d2.loss_cls: 0.4014  decode.d2.loss_mask: 0.6495  decode.d2.loss_dice: 0.5393  decode.d3.loss_cls: 0.5190  decode.d3.loss_mask: 0.5329  decode.d3.loss_dice: 0.4091  decode.d4.loss_cls: 0.3609  decode.d4.loss_mask: 0.7106  decode.d4.loss_dice: 0.5831  decode.d5.loss_cls: 0.3351  decode.d5.loss_mask: 1.0020  decode.d5.loss_dice: 0.6954  decode.d6.loss_cls: 0.3565  decode.d6.loss_mask: 0.9042  decode.d6.loss_dice: 0.6592  decode.d7.loss_cls: 0.3228  decode.d7.loss_mask: 0.8795  decode.d7.loss_dice: 0.6407  decode.d8.loss_cls: 0.3742  decode.d8.loss_mask: 0.8683  decode.d8.loss_dice: 0.6039
2023/05/23 07:41:29 - mmengine - INFO - Epoch(train) [1][2700/1297]  lr: 1.0000e-04  eta: 0:47:04  time: 0.9041  data_time: 0.1383  memory: 2829  grad_norm: 684.5186  loss: 15.0000  decode.loss_cls: 0.1975  decode.loss_mask: 0.8709  decode.loss_dice: 0.4290  decode.d0.loss_cls: 0.4398  decode.d0.loss_mask: 0.7007  decode.d0.loss_dice: 0.3705  decode.d1.loss_cls: 0.2449  decode.d1.loss_mask: 0.7533  decode.d1.loss_dice: 0.4180  decode.d2.loss_cls: 0.2715  decode.d2.loss_mask: 0.8401  decode.d2.loss_dice: 0.5150  decode.d3.loss_cls: 0.2231  decode.d3.loss_mask: 0.8342  decode.d3.loss_dice: 0.4960  decode.d4.loss_cls: 0.1364  decode.d4.loss_mask: 0.9683  decode.d4.loss_dice: 0.5766  decode.d5.loss_cls: 0.2450  decode.d5.loss_mask: 0.7859  decode.d5.loss_dice: 0.4364  decode.d6.loss_cls: 0.2232  decode.d6.loss_mask: 0.7683  decode.d6.loss_dice: 0.4223  decode.d7.loss_cls: 0.2027  decode.d7.loss_mask: 0.7805  decode.d7.loss_dice: 0.4191  decode.d8.loss_cls: 0.2434  decode.d8.loss_mask: 0.7641  decode.d8.loss_dice: 0.4235
2023/05/23 07:42:03 - mmengine - INFO - Epoch(train) [1][2750/1297]  lr: 1.0000e-04  eta: 0:46:22  time: 0.6034  data_time: 0.0084  memory: 2830  grad_norm: 68.7952  loss: 10.3904  decode.loss_cls: 0.1341  decode.loss_mask: 0.6943  decode.loss_dice: 0.2686  decode.d0.loss_cls: 0.0929  decode.d0.loss_mask: 0.6759  decode.d0.loss_dice: 0.2416  decode.d1.loss_cls: 0.1357  decode.d1.loss_mask: 0.6293  decode.d1.loss_dice: 0.2659  decode.d2.loss_cls: 0.1271  decode.d2.loss_mask: 0.7070  decode.d2.loss_dice: 0.2804  decode.d3.loss_cls: 0.1040  decode.d3.loss_mask: 0.6685  decode.d3.loss_dice: 0.2468  decode.d4.loss_cls: 0.1033  decode.d4.loss_mask: 0.6689  decode.d4.loss_dice: 0.2436  decode.d5.loss_cls: 0.1019  decode.d5.loss_mask: 0.6745  decode.d5.loss_dice: 0.2405  decode.d6.loss_cls: 0.1123  decode.d6.loss_mask: 0.6706  decode.d6.loss_dice: 0.2509  decode.d7.loss_cls: 0.1166  decode.d7.loss_mask: 0.6696  decode.d7.loss_dice: 0.2387  decode.d8.loss_cls: 0.1132  decode.d8.loss_mask: 0.6712  decode.d8.loss_dice: 0.2426
2023/05/23 07:42:03 - mmengine - INFO - Saving checkpoint at 2750 iterations
2023/05/23 07:42:43 - mmengine - INFO - Epoch(train) [1][2800/1297]  lr: 1.0000e-04  eta: 0:45:47  time: 0.7763  data_time: 0.0556  memory: 2830  grad_norm: 259.2429  loss: 17.0787  decode.loss_cls: 0.2932  decode.loss_mask: 0.9327  decode.loss_dice: 0.4502  decode.d0.loss_cls: 0.4559  decode.d0.loss_mask: 0.8908  decode.d0.loss_dice: 0.4289  decode.d1.loss_cls: 0.4014  decode.d1.loss_mask: 0.9535  decode.d1.loss_dice: 0.3997  decode.d2.loss_cls: 0.2620  decode.d2.loss_mask: 0.9331  decode.d2.loss_dice: 0.4611  decode.d3.loss_cls: 0.2842  decode.d3.loss_mask: 0.9805  decode.d3.loss_dice: 0.4183  decode.d4.loss_cls: 0.3130  decode.d4.loss_mask: 0.9257  decode.d4.loss_dice: 0.4345  decode.d5.loss_cls: 0.3257  decode.d5.loss_mask: 0.9476  decode.d5.loss_dice: 0.4284  decode.d6.loss_cls: 0.3164  decode.d6.loss_mask: 0.9696  decode.d6.loss_dice: 0.4644  decode.d7.loss_cls: 0.3034  decode.d7.loss_mask: 0.9542  decode.d7.loss_dice: 0.4586  decode.d8.loss_cls: 0.2906  decode.d8.loss_mask: 0.9435  decode.d8.loss_dice: 0.4575
2023/05/23 07:43:17 - mmengine - INFO - Epoch(train) [1][2850/1297]  lr: 1.0000e-04  eta: 0:45:06  time: 0.7488  data_time: 0.0099  memory: 2829  grad_norm: 610.9746  loss: 34.3099  decode.loss_cls: 0.3141  decode.loss_mask: 4.4725  decode.loss_dice: 1.0520  decode.d0.loss_cls: 0.5109  decode.d0.loss_mask: 1.1883  decode.d0.loss_dice: 0.8647  decode.d1.loss_cls: 0.5243  decode.d1.loss_mask: 1.2652  decode.d1.loss_dice: 0.8120  decode.d2.loss_cls: 0.6413  decode.d2.loss_mask: 1.1488  decode.d2.loss_dice: 0.9531  decode.d3.loss_cls: 0.4968  decode.d3.loss_mask: 1.2709  decode.d3.loss_dice: 0.9632  decode.d4.loss_cls: 0.5018  decode.d4.loss_mask: 1.2535  decode.d4.loss_dice: 0.8985  decode.d5.loss_cls: 0.5159  decode.d5.loss_mask: 1.3238  decode.d5.loss_dice: 0.9524  decode.d6.loss_cls: 0.5789  decode.d6.loss_mask: 1.2810  decode.d6.loss_dice: 0.9293  decode.d7.loss_cls: 0.3882  decode.d7.loss_mask: 2.9919  decode.d7.loss_dice: 0.9391  decode.d8.loss_cls: 0.3983  decode.d8.loss_mask: 3.8273  decode.d8.loss_dice: 1.0514
2023/05/23 07:43:53 - mmengine - INFO - Epoch(train) [1][2900/1297]  lr: 1.0000e-04  eta: 0:44:28  time: 0.6305  data_time: 0.0098  memory: 2829  grad_norm: 284.2707  loss: 23.1428  decode.loss_cls: 0.2057  decode.loss_mask: 1.7393  decode.loss_dice: 0.6171  decode.d0.loss_cls: 0.3061  decode.d0.loss_mask: 1.0458  decode.d0.loss_dice: 0.6853  decode.d1.loss_cls: 0.3031  decode.d1.loss_mask: 1.0753  decode.d1.loss_dice: 0.5981  decode.d2.loss_cls: 0.3937  decode.d2.loss_mask: 1.1389  decode.d2.loss_dice: 0.7003  decode.d3.loss_cls: 0.2511  decode.d3.loss_mask: 1.4894  decode.d3.loss_dice: 0.6508  decode.d4.loss_cls: 0.3414  decode.d4.loss_mask: 1.2071  decode.d4.loss_dice: 0.6615  decode.d5.loss_cls: 0.3471  decode.d5.loss_mask: 1.1466  decode.d5.loss_dice: 0.6702  decode.d6.loss_cls: 0.2315  decode.d6.loss_mask: 1.4582  decode.d6.loss_dice: 0.6594  decode.d7.loss_cls: 0.2298  decode.d7.loss_mask: 1.8382  decode.d7.loss_dice: 0.6429  decode.d8.loss_cls: 0.2451  decode.d8.loss_mask: 1.4503  decode.d8.loss_dice: 0.8135
2023/05/23 07:44:28 - mmengine - INFO - Epoch(train) [1][2950/1297]  lr: 1.0000e-04  eta: 0:43:48  time: 0.5520  data_time: 0.0073  memory: 2829  grad_norm: 282.7904  loss: 18.1187  decode.loss_cls: 0.1708  decode.loss_mask: 1.3106  decode.loss_dice: 0.5872  decode.d0.loss_cls: 0.2644  decode.d0.loss_mask: 1.0143  decode.d0.loss_dice: 0.5268  decode.d1.loss_cls: 0.2280  decode.d1.loss_mask: 1.0613  decode.d1.loss_dice: 0.4948  decode.d2.loss_cls: 0.2223  decode.d2.loss_mask: 0.9784  decode.d2.loss_dice: 0.5480  decode.d3.loss_cls: 0.2223  decode.d3.loss_mask: 1.1490  decode.d3.loss_dice: 0.6796  decode.d4.loss_cls: 0.2525  decode.d4.loss_mask: 0.7693  decode.d4.loss_dice: 0.5141  decode.d5.loss_cls: 0.2357  decode.d5.loss_mask: 0.8394  decode.d5.loss_dice: 0.4922  decode.d6.loss_cls: 0.2671  decode.d6.loss_mask: 0.7987  decode.d6.loss_dice: 0.5020  decode.d7.loss_cls: 0.2055  decode.d7.loss_mask: 1.1005  decode.d7.loss_dice: 0.5752  decode.d8.loss_cls: 0.1632  decode.d8.loss_mask: 1.3414  decode.d8.loss_dice: 0.6040
2023/05/23 07:45:06 - mmengine - INFO - Exp name: mask2former_r50_8xb2-160k_ade20k-512x512_20230523_070719
2023/05/23 07:45:06 - mmengine - INFO - Epoch(train) [1][3000/1297]  lr: 1.0000e-04  eta: 0:43:11  time: 0.9179  data_time: 0.0172  memory: 2829  grad_norm: 312.5420  loss: 23.4773  decode.loss_cls: 0.2009  decode.loss_mask: 1.4692  decode.loss_dice: 0.6792  decode.d0.loss_cls: 0.1477  decode.d0.loss_mask: 1.3541  decode.d0.loss_dice: 0.7345  decode.d1.loss_cls: 0.2301  decode.d1.loss_mask: 1.2670  decode.d1.loss_dice: 0.7005  decode.d2.loss_cls: 0.2838  decode.d2.loss_mask: 1.2095  decode.d2.loss_dice: 0.6782  decode.d3.loss_cls: 0.2423  decode.d3.loss_mask: 1.8058  decode.d3.loss_dice: 0.7562  decode.d4.loss_cls: 0.1811  decode.d4.loss_mask: 1.5766  decode.d4.loss_dice: 0.8257  decode.d5.loss_cls: 0.1440  decode.d5.loss_mask: 1.3579  decode.d5.loss_dice: 0.6876  decode.d6.loss_cls: 0.1319  decode.d6.loss_mask: 1.4070  decode.d6.loss_dice: 0.7642  decode.d7.loss_cls: 0.1642  decode.d7.loss_mask: 1.4078  decode.d7.loss_dice: 0.7513  decode.d8.loss_cls: 0.2101  decode.d8.loss_mask: 1.4356  decode.d8.loss_dice: 0.6736
2023/05/23 07:45:06 - mmengine - INFO - Saving checkpoint at 3000 iterations
2023/05/23 07:45:44 - mmengine - INFO - Epoch(train) [1][3050/1297]  lr: 1.0000e-04  eta: 0:42:36  time: 0.6375  data_time: 0.0073  memory: 2829  grad_norm: 440.8228  loss: 21.6603  decode.loss_cls: 0.3759  decode.loss_mask: 0.9193  decode.loss_dice: 0.6000  decode.d0.loss_cls: 0.4364  decode.d0.loss_mask: 0.8045  decode.d0.loss_dice: 0.5208  decode.d1.loss_cls: 0.3158  decode.d1.loss_mask: 1.0806  decode.d1.loss_dice: 0.7257  decode.d2.loss_cls: 0.3263  decode.d2.loss_mask: 1.6718  decode.d2.loss_dice: 0.7224  decode.d3.loss_cls: 0.4145  decode.d3.loss_mask: 0.9169  decode.d3.loss_dice: 0.6106  decode.d4.loss_cls: 0.3973  decode.d4.loss_mask: 1.0201  decode.d4.loss_dice: 0.7733  decode.d5.loss_cls: 0.4258  decode.d5.loss_mask: 0.8875  decode.d5.loss_dice: 0.5803  decode.d6.loss_cls: 0.3235  decode.d6.loss_mask: 1.9942  decode.d6.loss_dice: 0.7543  decode.d7.loss_cls: 0.4488  decode.d7.loss_mask: 0.9700  decode.d7.loss_dice: 0.6947  decode.d8.loss_cls: 0.3642  decode.d8.loss_mask: 0.9446  decode.d8.loss_dice: 0.6401
2023/05/23 07:46:18 - mmengine - INFO - Epoch(train) [1][3100/1297]  lr: 1.0000e-04  eta: 0:41:55  time: 0.6287  data_time: 0.0090  memory: 2829  grad_norm: 202.1891  loss: 16.8166  decode.loss_cls: 0.1338  decode.loss_mask: 1.0283  decode.loss_dice: 0.5223  decode.d0.loss_cls: 0.3016  decode.d0.loss_mask: 0.7434  decode.d0.loss_dice: 0.4568  decode.d1.loss_cls: 0.2034  decode.d1.loss_mask: 1.0059  decode.d1.loss_dice: 0.5303  decode.d2.loss_cls: 0.1795  decode.d2.loss_mask: 1.0226  decode.d2.loss_dice: 0.5399  decode.d3.loss_cls: 0.2608  decode.d3.loss_mask: 0.8425  decode.d3.loss_dice: 0.5294  decode.d4.loss_cls: 0.1667  decode.d4.loss_mask: 1.0251  decode.d4.loss_dice: 0.5467  decode.d5.loss_cls: 0.1801  decode.d5.loss_mask: 1.0044  decode.d5.loss_dice: 0.5363  decode.d6.loss_cls: 0.1408  decode.d6.loss_mask: 1.0265  decode.d6.loss_dice: 0.5307  decode.d7.loss_cls: 0.1292  decode.d7.loss_mask: 1.0150  decode.d7.loss_dice: 0.5182  decode.d8.loss_cls: 0.1486  decode.d8.loss_mask: 1.0194  decode.d8.loss_dice: 0.5283
2023/05/23 07:46:55 - mmengine - INFO - Epoch(train) [1][3150/1297]  lr: 1.0000e-04  eta: 0:41:18  time: 0.6761  data_time: 0.0339  memory: 2829  grad_norm: 457.8879  loss: 25.4422  decode.loss_cls: 0.4471  decode.loss_mask: 1.2784  decode.loss_dice: 0.6271  decode.d0.loss_cls: 0.3081  decode.d0.loss_mask: 1.2423  decode.d0.loss_dice: 0.6092  decode.d1.loss_cls: 0.4262  decode.d1.loss_mask: 1.0657  decode.d1.loss_dice: 0.6460  decode.d2.loss_cls: 0.4037  decode.d2.loss_mask: 1.1904  decode.d2.loss_dice: 0.6035  decode.d3.loss_cls: 0.4248  decode.d3.loss_mask: 1.1711  decode.d3.loss_dice: 0.6696  decode.d4.loss_cls: 0.4547  decode.d4.loss_mask: 1.3135  decode.d4.loss_dice: 0.7520  decode.d5.loss_cls: 0.3965  decode.d5.loss_mask: 1.4879  decode.d5.loss_dice: 0.7945  decode.d6.loss_cls: 0.3365  decode.d6.loss_mask: 2.4366  decode.d6.loss_dice: 0.9155  decode.d7.loss_cls: 0.3303  decode.d7.loss_mask: 1.9716  decode.d7.loss_dice: 0.7795  decode.d8.loss_cls: 0.4246  decode.d8.loss_mask: 1.3018  decode.d8.loss_dice: 0.6336
2023/05/23 07:47:35 - mmengine - INFO - Epoch(train) [1][3200/1297]  lr: 1.0000e-04  eta: 0:40:43  time: 0.8523  data_time: 0.0722  memory: 2829  grad_norm: 319.5097  loss: 19.4245  decode.loss_cls: 0.2347  decode.loss_mask: 1.5613  decode.loss_dice: 0.4903  decode.d0.loss_cls: 0.2522  decode.d0.loss_mask: 1.0340  decode.d0.loss_dice: 0.4961  decode.d1.loss_cls: 0.2478  decode.d1.loss_mask: 1.0949  decode.d1.loss_dice: 0.4901  decode.d2.loss_cls: 0.1762  decode.d2.loss_mask: 1.1831  decode.d2.loss_dice: 0.4769  decode.d3.loss_cls: 0.2934  decode.d3.loss_mask: 1.2395  decode.d3.loss_dice: 0.4664  decode.d4.loss_cls: 0.2844  decode.d4.loss_mask: 1.2451  decode.d4.loss_dice: 0.4969  decode.d5.loss_cls: 0.2162  decode.d5.loss_mask: 1.1215  decode.d5.loss_dice: 0.4741  decode.d6.loss_cls: 0.2062  decode.d6.loss_mask: 1.2559  decode.d6.loss_dice: 0.4245  decode.d7.loss_cls: 0.2091  decode.d7.loss_mask: 1.1192  decode.d7.loss_dice: 0.4758  decode.d8.loss_cls: 0.1333  decode.d8.loss_mask: 1.4343  decode.d8.loss_dice: 0.5912
2023/05/23 07:48:14 - mmengine - INFO - Epoch(train) [1][3250/1297]  lr: 1.0000e-04  eta: 0:40:08  time: 0.6933  data_time: 0.0746  memory: 2829  grad_norm: 89.7646  loss: 15.7726  decode.loss_cls: 0.0985  decode.loss_mask: 0.9930  decode.loss_dice: 0.5969  decode.d0.loss_cls: 0.1460  decode.d0.loss_mask: 0.7053  decode.d0.loss_dice: 0.5088  decode.d1.loss_cls: 0.1648  decode.d1.loss_mask: 0.6664  decode.d1.loss_dice: 0.5030  decode.d2.loss_cls: 0.1896  decode.d2.loss_mask: 0.7520  decode.d2.loss_dice: 0.5398  decode.d3.loss_cls: 0.1642  decode.d3.loss_mask: 0.7110  decode.d3.loss_dice: 0.5765  decode.d4.loss_cls: 0.1500  decode.d4.loss_mask: 0.9192  decode.d4.loss_dice: 0.6246  decode.d5.loss_cls: 0.0974  decode.d5.loss_mask: 1.0200  decode.d5.loss_dice: 0.5317  decode.d6.loss_cls: 0.1034  decode.d6.loss_mask: 0.9738  decode.d6.loss_dice: 0.6340  decode.d7.loss_cls: 0.0981  decode.d7.loss_mask: 0.9852  decode.d7.loss_dice: 0.6259  decode.d8.loss_cls: 0.0852  decode.d8.loss_mask: 1.0344  decode.d8.loss_dice: 0.5736
2023/05/23 07:48:14 - mmengine - INFO - Saving checkpoint at 3250 iterations
2023/05/23 07:48:55 - mmengine - INFO - Epoch(train) [1][3300/1297]  lr: 1.0000e-04  eta: 0:39:34  time: 0.6768  data_time: 0.0094  memory: 2829  grad_norm: 127.4739  loss: 17.1020  decode.loss_cls: 0.1603  decode.loss_mask: 0.9851  decode.loss_dice: 0.5326  decode.d0.loss_cls: 0.2341  decode.d0.loss_mask: 1.1336  decode.d0.loss_dice: 0.5907  decode.d1.loss_cls: 0.1221  decode.d1.loss_mask: 1.0270  decode.d1.loss_dice: 0.4992  decode.d2.loss_cls: 0.2315  decode.d2.loss_mask: 0.9226  decode.d2.loss_dice: 0.5516  decode.d3.loss_cls: 0.3137  decode.d3.loss_mask: 1.0115  decode.d3.loss_dice: 0.5677  decode.d4.loss_cls: 0.2050  decode.d4.loss_mask: 1.0136  decode.d4.loss_dice: 0.5379  decode.d5.loss_cls: 0.1771  decode.d5.loss_mask: 0.9731  decode.d5.loss_dice: 0.5002  decode.d6.loss_cls: 0.1839  decode.d6.loss_mask: 0.8835  decode.d6.loss_dice: 0.4814  decode.d7.loss_cls: 0.1401  decode.d7.loss_mask: 0.9752  decode.d7.loss_dice: 0.5121  decode.d8.loss_cls: 0.1641  decode.d8.loss_mask: 0.9657  decode.d8.loss_dice: 0.5060
2023/05/23 07:49:28 - mmengine - INFO - Epoch(train) [1][3350/1297]  lr: 1.0000e-04  eta: 0:38:53  time: 0.7154  data_time: 0.0116  memory: 2829  grad_norm: 185.6656  loss: 13.3559  decode.loss_cls: 0.1286  decode.loss_mask: 1.4830  decode.loss_dice: 0.7111  decode.d0.loss_cls: 0.4322  decode.d0.loss_mask: 0.4125  decode.d0.loss_dice: 0.2710  decode.d1.loss_cls: 0.3603  decode.d1.loss_mask: 0.4662  decode.d1.loss_dice: 0.3016  decode.d2.loss_cls: 0.3392  decode.d2.loss_mask: 0.4232  decode.d2.loss_dice: 0.2694  decode.d3.loss_cls: 0.2980  decode.d3.loss_mask: 0.4383  decode.d3.loss_dice: 0.2880  decode.d4.loss_cls: 0.2043  decode.d4.loss_mask: 0.6087  decode.d4.loss_dice: 0.3861  decode.d5.loss_cls: 0.1778  decode.d5.loss_mask: 0.6706  decode.d5.loss_dice: 0.4018  decode.d6.loss_cls: 0.1526  decode.d6.loss_mask: 0.7285  decode.d6.loss_dice: 0.3833  decode.d7.loss_cls: 0.2295  decode.d7.loss_mask: 0.8428  decode.d7.loss_dice: 0.4684  decode.d8.loss_cls: 0.3098  decode.d8.loss_mask: 0.7356  decode.d8.loss_dice: 0.4336
2023/05/23 07:50:04 - mmengine - INFO - Epoch(train) [1][3400/1297]  lr: 1.0000e-04  eta: 0:38:15  time: 0.6135  data_time: 0.0076  memory: 2830  grad_norm: 234.2570  loss: 21.3184  decode.loss_cls: 0.3145  decode.loss_mask: 1.2202  decode.loss_dice: 0.7365  decode.d0.loss_cls: 0.3796  decode.d0.loss_mask: 0.9193  decode.d0.loss_dice: 0.6492  decode.d1.loss_cls: 0.3132  decode.d1.loss_mask: 0.8906  decode.d1.loss_dice: 0.5996  decode.d2.loss_cls: 0.2394  decode.d2.loss_mask: 0.9311  decode.d2.loss_dice: 0.8106  decode.d3.loss_cls: 0.3013  decode.d3.loss_mask: 1.1050  decode.d3.loss_dice: 0.8129  decode.d4.loss_cls: 0.2098  decode.d4.loss_mask: 1.1880  decode.d4.loss_dice: 0.8507  decode.d5.loss_cls: 0.2647  decode.d5.loss_mask: 1.1208  decode.d5.loss_dice: 0.8263  decode.d6.loss_cls: 0.3266  decode.d6.loss_mask: 1.0658  decode.d6.loss_dice: 0.7423  decode.d7.loss_cls: 0.3033  decode.d7.loss_mask: 1.1394  decode.d7.loss_dice: 0.7504  decode.d8.loss_cls: 0.3715  decode.d8.loss_mask: 1.2215  decode.d8.loss_dice: 0.7143
2023/05/23 07:50:43 - mmengine - INFO - Epoch(train) [1][3450/1297]  lr: 1.0000e-04  eta: 0:37:39  time: 0.8056  data_time: 0.0103  memory: 2835  grad_norm: 93.7642  loss: 13.2107  decode.loss_cls: 0.2633  decode.loss_mask: 0.6062  decode.loss_dice: 0.4781  decode.d0.loss_cls: 0.2323  decode.d0.loss_mask: 0.4592  decode.d0.loss_dice: 0.3487  decode.d1.loss_cls: 0.2538  decode.d1.loss_mask: 0.4860  decode.d1.loss_dice: 0.3833  decode.d2.loss_cls: 0.2914  decode.d2.loss_mask: 0.5438  decode.d2.loss_dice: 0.4538  decode.d3.loss_cls: 0.2895  decode.d3.loss_mask: 0.5231  decode.d3.loss_dice: 0.4571  decode.d4.loss_cls: 0.3115  decode.d4.loss_mask: 0.8211  decode.d4.loss_dice: 0.5363  decode.d5.loss_cls: 0.3069  decode.d5.loss_mask: 1.0364  decode.d5.loss_dice: 0.4812  decode.d6.loss_cls: 0.2900  decode.d6.loss_mask: 0.5581  decode.d6.loss_dice: 0.4106  decode.d7.loss_cls: 0.2883  decode.d7.loss_mask: 0.5169  decode.d7.loss_dice: 0.3919  decode.d8.loss_cls: 0.2710  decode.d8.loss_mask: 0.5290  decode.d8.loss_dice: 0.3918
2023/05/23 07:51:18 - mmengine - INFO - Epoch(train) [1][3500/1297]  lr: 1.0000e-04  eta: 0:37:00  time: 0.8164  data_time: 0.0585  memory: 2829  grad_norm: 142.8095  loss: 17.1001  decode.loss_cls: 0.1642  decode.loss_mask: 1.0964  decode.loss_dice: 0.3377  decode.d0.loss_cls: 0.3321  decode.d0.loss_mask: 1.0693  decode.d0.loss_dice: 0.4312  decode.d1.loss_cls: 0.1874  decode.d1.loss_mask: 1.2131  decode.d1.loss_dice: 0.4274  decode.d2.loss_cls: 0.1885  decode.d2.loss_mask: 1.1503  decode.d2.loss_dice: 0.4276  decode.d3.loss_cls: 0.1630  decode.d3.loss_mask: 1.0882  decode.d3.loss_dice: 0.3533  decode.d4.loss_cls: 0.2726  decode.d4.loss_mask: 1.2551  decode.d4.loss_dice: 0.4439  decode.d5.loss_cls: 0.1953  decode.d5.loss_mask: 1.0709  decode.d5.loss_dice: 0.3576  decode.d6.loss_cls: 0.2070  decode.d6.loss_mask: 1.0923  decode.d6.loss_dice: 0.3372  decode.d7.loss_cls: 0.2015  decode.d7.loss_mask: 1.1037  decode.d7.loss_dice: 0.3448  decode.d8.loss_cls: 0.1646  decode.d8.loss_mask: 1.0824  decode.d8.loss_dice: 0.3413
2023/05/23 07:51:18 - mmengine - INFO - Saving checkpoint at 3500 iterations
2023/05/23 07:52:03 - mmengine - INFO - Epoch(train) [1][3550/1297]  lr: 1.0000e-04  eta: 0:36:29  time: 0.7590  data_time: 0.0108  memory: 2829  grad_norm: 444.3755  loss: 28.4093  decode.loss_cls: 0.3463  decode.loss_mask: 1.6967  decode.loss_dice: 0.8203  decode.d0.loss_cls: 0.6457  decode.d0.loss_mask: 1.1165  decode.d0.loss_dice: 0.7225  decode.d1.loss_cls: 0.6212  decode.d1.loss_mask: 1.2266  decode.d1.loss_dice: 0.7806  decode.d2.loss_cls: 0.3508  decode.d2.loss_mask: 1.7516  decode.d2.loss_dice: 0.8469  decode.d3.loss_cls: 0.2685  decode.d3.loss_mask: 1.8648  decode.d3.loss_dice: 1.0126  decode.d4.loss_cls: 0.4689  decode.d4.loss_mask: 1.6609  decode.d4.loss_dice: 0.8211  decode.d5.loss_cls: 0.5182  decode.d5.loss_mask: 1.7472  decode.d5.loss_dice: 0.7682  decode.d6.loss_cls: 0.3842  decode.d6.loss_mask: 1.6987  decode.d6.loss_dice: 0.7269  decode.d7.loss_cls: 0.3306  decode.d7.loss_mask: 1.6457  decode.d7.loss_dice: 0.7040  decode.d8.loss_cls: 0.4096  decode.d8.loss_mask: 1.6602  decode.d8.loss_dice: 0.7934
2023/05/23 07:52:39 - mmengine - INFO - Epoch(train) [1][3600/1297]  lr: 1.0000e-04  eta: 0:35:51  time: 0.5992  data_time: 0.0073  memory: 2829  grad_norm: 220.1020  loss: 23.4589  decode.loss_cls: 0.2476  decode.loss_mask: 1.4617  decode.loss_dice: 0.6824  decode.d0.loss_cls: 0.2626  decode.d0.loss_mask: 1.3389  decode.d0.loss_dice: 0.5946  decode.d1.loss_cls: 0.2030  decode.d1.loss_mask: 1.2512  decode.d1.loss_dice: 0.5129  decode.d2.loss_cls: 0.1483  decode.d2.loss_mask: 1.3541  decode.d2.loss_dice: 0.6755  decode.d3.loss_cls: 0.2689  decode.d3.loss_mask: 1.2695  decode.d3.loss_dice: 0.5660  decode.d4.loss_cls: 0.2476  decode.d4.loss_mask: 1.5518  decode.d4.loss_dice: 0.6674  decode.d5.loss_cls: 0.2414  decode.d5.loss_mask: 1.8849  decode.d5.loss_dice: 0.6734  decode.d6.loss_cls: 0.2794  decode.d6.loss_mask: 1.6881  decode.d6.loss_dice: 0.6924  decode.d7.loss_cls: 0.2436  decode.d7.loss_mask: 1.3801  decode.d7.loss_dice: 0.7345  decode.d8.loss_cls: 0.2148  decode.d8.loss_mask: 1.4590  decode.d8.loss_dice: 0.6633
2023/05/23 07:53:13 - mmengine - INFO - Epoch(train) [1][3650/1297]  lr: 1.0000e-04  eta: 0:35:11  time: 0.5504  data_time: 0.0073  memory: 2829  grad_norm: 249.1891  loss: 23.9712  decode.loss_cls: 0.4372  decode.loss_mask: 1.2921  decode.loss_dice: 0.8146  decode.d0.loss_cls: 0.4755  decode.d0.loss_mask: 1.1301  decode.d0.loss_dice: 0.6625  decode.d1.loss_cls: 0.4319  decode.d1.loss_mask: 1.0797  decode.d1.loss_dice: 0.6800  decode.d2.loss_cls: 0.4659  decode.d2.loss_mask: 1.0241  decode.d2.loss_dice: 0.6877  decode.d3.loss_cls: 0.4922  decode.d3.loss_mask: 1.2913  decode.d3.loss_dice: 0.7812  decode.d4.loss_cls: 0.5725  decode.d4.loss_mask: 1.0732  decode.d4.loss_dice: 0.7292  decode.d5.loss_cls: 0.4245  decode.d5.loss_mask: 1.3056  decode.d5.loss_dice: 0.8189  decode.d6.loss_cls: 0.3473  decode.d6.loss_mask: 1.2881  decode.d6.loss_dice: 0.7373  decode.d7.loss_cls: 0.3915  decode.d7.loss_mask: 1.2856  decode.d7.loss_dice: 0.7660  decode.d8.loss_cls: 0.4263  decode.d8.loss_mask: 1.2953  decode.d8.loss_dice: 0.7641
2023/05/23 07:53:49 - mmengine - INFO - Epoch(train) [1][3700/1297]  lr: 1.0000e-04  eta: 0:34:33  time: 0.6758  data_time: 0.0209  memory: 2830  grad_norm: 499.9169  loss: 27.8900  decode.loss_cls: 0.3275  decode.loss_mask: 2.0917  decode.loss_dice: 0.8100  decode.d0.loss_cls: 0.2751  decode.d0.loss_mask: 1.1325  decode.d0.loss_dice: 0.7347  decode.d1.loss_cls: 0.2713  decode.d1.loss_mask: 1.1935  decode.d1.loss_dice: 0.7568  decode.d2.loss_cls: 0.3454  decode.d2.loss_mask: 1.2146  decode.d2.loss_dice: 0.7614  decode.d3.loss_cls: 0.1186  decode.d3.loss_mask: 2.7949  decode.d3.loss_dice: 0.9715  decode.d4.loss_cls: 0.1271  decode.d4.loss_mask: 2.8040  decode.d4.loss_dice: 0.9490  decode.d5.loss_cls: 0.3330  decode.d5.loss_mask: 1.1705  decode.d5.loss_dice: 0.7807  decode.d6.loss_cls: 0.2784  decode.d6.loss_mask: 1.1283  decode.d6.loss_dice: 0.7895  decode.d7.loss_cls: 0.2564  decode.d7.loss_mask: 1.1587  decode.d7.loss_dice: 0.7915  decode.d8.loss_cls: 0.2063  decode.d8.loss_mask: 2.4011  decode.d8.loss_dice: 0.9160
2023/05/23 07:54:25 - mmengine - INFO - Epoch(train) [1][3750/1297]  lr: 1.0000e-04  eta: 0:33:55  time: 0.8415  data_time: 0.0170  memory: 2829  grad_norm: 290.6860  loss: 23.5139  decode.loss_cls: 0.2143  decode.loss_mask: 1.5182  decode.loss_dice: 0.6504  decode.d0.loss_cls: 0.3290  decode.d0.loss_mask: 1.3083  decode.d0.loss_dice: 0.5974  decode.d1.loss_cls: 0.3052  decode.d1.loss_mask: 1.3184  decode.d1.loss_dice: 0.6242  decode.d2.loss_cls: 0.2771  decode.d2.loss_mask: 1.2504  decode.d2.loss_dice: 0.5904  decode.d3.loss_cls: 0.1450  decode.d3.loss_mask: 1.5581  decode.d3.loss_dice: 0.6771  decode.d4.loss_cls: 0.2260  decode.d4.loss_mask: 1.5472  decode.d4.loss_dice: 0.6817  decode.d5.loss_cls: 0.2158  decode.d5.loss_mask: 1.4990  decode.d5.loss_dice: 0.6659  decode.d6.loss_cls: 0.2577  decode.d6.loss_mask: 1.5667  decode.d6.loss_dice: 0.6976  decode.d7.loss_cls: 0.2170  decode.d7.loss_mask: 1.5133  decode.d7.loss_dice: 0.6773  decode.d8.loss_cls: 0.2734  decode.d8.loss_mask: 1.4661  decode.d8.loss_dice: 0.6459
2023/05/23 07:54:25 - mmengine - INFO - Saving checkpoint at 3750 iterations
2023/05/23 07:55:05 - mmengine - INFO - Epoch(train) [1][3800/1297]  lr: 1.0000e-04  eta: 0:33:20  time: 0.6253  data_time: 0.0098  memory: 2829  grad_norm: 218.3052  loss: 16.4771  decode.loss_cls: 0.2390  decode.loss_mask: 0.9245  decode.loss_dice: 0.4489  decode.d0.loss_cls: 0.2232  decode.d0.loss_mask: 0.7496  decode.d0.loss_dice: 0.3754  decode.d1.loss_cls: 0.1764  decode.d1.loss_mask: 2.2432  decode.d1.loss_dice: 0.4946  decode.d2.loss_cls: 0.2176  decode.d2.loss_mask: 0.7947  decode.d2.loss_dice: 0.3629  decode.d3.loss_cls: 0.2438  decode.d3.loss_mask: 0.7944  decode.d3.loss_dice: 0.3953  decode.d4.loss_cls: 0.2701  decode.d4.loss_mask: 0.7995  decode.d4.loss_dice: 0.3776  decode.d5.loss_cls: 0.2788  decode.d5.loss_mask: 0.7749  decode.d5.loss_dice: 0.3840  decode.d6.loss_cls: 0.2540  decode.d6.loss_mask: 0.8182  decode.d6.loss_dice: 0.4017  decode.d7.loss_cls: 0.2669  decode.d7.loss_mask: 0.8102  decode.d7.loss_dice: 0.3724  decode.d8.loss_cls: 0.2677  decode.d8.loss_mask: 1.2742  decode.d8.loss_dice: 0.4434
2023/05/23 07:55:42 - mmengine - INFO - Epoch(train) [1][3850/1297]  lr: 1.0000e-04  eta: 0:32:42  time: 0.6659  data_time: 0.0104  memory: 2829  grad_norm: 453.3558  loss: 19.2368  decode.loss_cls: 0.2088  decode.loss_mask: 0.8260  decode.loss_dice: 0.5771  decode.d0.loss_cls: 0.2959  decode.d0.loss_mask: 0.9159  decode.d0.loss_dice: 0.5978  decode.d1.loss_cls: 0.2039  decode.d1.loss_mask: 0.9923  decode.d1.loss_dice: 0.5487  decode.d2.loss_cls: 0.2293  decode.d2.loss_mask: 0.8363  decode.d2.loss_dice: 0.5373  decode.d3.loss_cls: 0.3535  decode.d3.loss_mask: 0.7910  decode.d3.loss_dice: 0.5007  decode.d4.loss_cls: 0.2864  decode.d4.loss_mask: 0.8028  decode.d4.loss_dice: 0.5184  decode.d5.loss_cls: 0.2742  decode.d5.loss_mask: 0.9382  decode.d5.loss_dice: 0.5371  decode.d6.loss_cls: 0.3316  decode.d6.loss_mask: 0.7386  decode.d6.loss_dice: 0.4874  decode.d7.loss_cls: 0.2604  decode.d7.loss_mask: 2.4295  decode.d7.loss_dice: 0.7667  decode.d8.loss_cls: 0.2037  decode.d8.loss_mask: 1.5164  decode.d8.loss_dice: 0.7313
2023/05/23 07:56:18 - mmengine - INFO - Epoch(train) [1][3900/1297]  lr: 1.0000e-04  eta: 0:32:04  time: 0.7129  data_time: 0.0118  memory: 2829  grad_norm: 177.4532  loss: 20.3966  decode.loss_cls: 0.2284  decode.loss_mask: 1.1543  decode.loss_dice: 0.6261  decode.d0.loss_cls: 0.1619  decode.d0.loss_mask: 1.2918  decode.d0.loss_dice: 0.6667  decode.d1.loss_cls: 0.1842  decode.d1.loss_mask: 1.1588  decode.d1.loss_dice: 0.5956  decode.d2.loss_cls: 0.2058  decode.d2.loss_mask: 1.1746  decode.d2.loss_dice: 0.7015  decode.d3.loss_cls: 0.2438  decode.d3.loss_mask: 1.1060  decode.d3.loss_dice: 0.6699  decode.d4.loss_cls: 0.2197  decode.d4.loss_mask: 1.1748  decode.d4.loss_dice: 0.6517  decode.d5.loss_cls: 0.2361  decode.d5.loss_mask: 1.1807  decode.d5.loss_dice: 0.6563  decode.d6.loss_cls: 0.2813  decode.d6.loss_mask: 1.1031  decode.d6.loss_dice: 0.6547  decode.d7.loss_cls: 0.2085  decode.d7.loss_mask: 1.2063  decode.d7.loss_dice: 0.6300  decode.d8.loss_cls: 0.2369  decode.d8.loss_mask: 1.1435  decode.d8.loss_dice: 0.6438
2023/05/23 07:56:56 - mmengine - INFO - Epoch(train) [1][3950/1297]  lr: 1.0000e-04  eta: 0:31:27  time: 0.8665  data_time: 0.0555  memory: 2829  grad_norm: 288.0614  loss: 23.0028  decode.loss_cls: 0.1860  decode.loss_mask: 1.3162  decode.loss_dice: 0.6688  decode.d0.loss_cls: 0.3182  decode.d0.loss_mask: 1.4182  decode.d0.loss_dice: 0.6413  decode.d1.loss_cls: 0.1928  decode.d1.loss_mask: 1.4569  decode.d1.loss_dice: 0.7522  decode.d2.loss_cls: 0.1581  decode.d2.loss_mask: 1.4401  decode.d2.loss_dice: 0.6798  decode.d3.loss_cls: 0.1676  decode.d3.loss_mask: 1.4633  decode.d3.loss_dice: 0.6452  decode.d4.loss_cls: 0.0962  decode.d4.loss_mask: 1.3515  decode.d4.loss_dice: 0.6369  decode.d5.loss_cls: 0.0931  decode.d5.loss_mask: 1.3677  decode.d5.loss_dice: 0.6681  decode.d6.loss_cls: 0.1816  decode.d6.loss_mask: 1.4855  decode.d6.loss_dice: 0.7812  decode.d7.loss_cls: 0.1544  decode.d7.loss_mask: 1.6337  decode.d7.loss_dice: 0.8205  decode.d8.loss_cls: 0.1326  decode.d8.loss_mask: 1.3746  decode.d8.loss_dice: 0.7203
2023/05/23 07:57:33 - mmengine - INFO - Exp name: mask2former_r50_8xb2-160k_ade20k-512x512_20230523_070719
2023/05/23 07:57:33 - mmengine - INFO - Epoch(train) [1][4000/1297]  lr: 1.0000e-04  eta: 0:30:50  time: 0.6413  data_time: 0.0094  memory: 2829  grad_norm: 164.3596  loss: 21.0045  decode.loss_cls: 0.0861  decode.loss_mask: 1.3206  decode.loss_dice: 0.5568  decode.d0.loss_cls: 0.2556  decode.d0.loss_mask: 1.2915  decode.d0.loss_dice: 0.5091  decode.d1.loss_cls: 0.1807  decode.d1.loss_mask: 1.4393  decode.d1.loss_dice: 0.7136  decode.d2.loss_cls: 0.0623  decode.d2.loss_mask: 1.5302  decode.d2.loss_dice: 0.6123  decode.d3.loss_cls: 0.0887  decode.d3.loss_mask: 1.3819  decode.d3.loss_dice: 0.5976  decode.d4.loss_cls: 0.0525  decode.d4.loss_mask: 1.5021  decode.d4.loss_dice: 0.6602  decode.d5.loss_cls: 0.0732  decode.d5.loss_mask: 1.3345  decode.d5.loss_dice: 0.5869  decode.d6.loss_cls: 0.1136  decode.d6.loss_mask: 1.3849  decode.d6.loss_dice: 0.5443  decode.d7.loss_cls: 0.0700  decode.d7.loss_mask: 1.4197  decode.d7.loss_dice: 0.6854  decode.d8.loss_cls: 0.0935  decode.d8.loss_mask: 1.3066  decode.d8.loss_dice: 0.5509
2023/05/23 07:57:33 - mmengine - INFO - Saving checkpoint at 4000 iterations
2023/05/23 07:58:39 - mmengine - WARNING - `Visualizer` backend is not initialized because save_dir is None.
